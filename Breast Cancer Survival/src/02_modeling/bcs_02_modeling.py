# -*- coding: utf-8 -*-
"""BCS_02_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_AxjRNMCUd72yCBeSqUhfaPoYeSzbY-
"""

# Import Libraries and Verify Dependencies

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
!pip install lifelines
from lifelines import CoxPHFitter

"""**Step#1** This initial step ensures that the Python environment is properly configured for survival analysis and clinical data processing. It begins by importing essential libraries: os for file operations, pandas for data manipulation, matplotlib.pyplot and seaborn for visualization, and sklearn.preprocessing for encoding and scaling features. The lifelines package, critical for survival modeling, is explicitly installed using !pip install lifelines, followed by a confirmation that all dependencies are already satisfied. This guarantees reproducibility and avoids runtime errors due to missing packages. The final import of CoxPHFitter from lifelines prepares the environment for fitting Cox proportional hazards models."""

#2 Mount Google Drive for Persistent Storage
from google.colab import drive
drive.mount('/content/drive')

"""**Step#2** This step establishes a persistent link between the Google Colab environment and the user's Google Drive, enabling seamless access to files stored in the cloud. By importing drive from google.colab and executing drive.mount('/content/drive'), the notebook prompts the user to authorize access to their Drive. Once authenticated, the Drive is mounted at /content/drive, allowing read/write operations on datasets, outputs, and saved models directly from Colab. This integration is essential for reproducibility, data persistence, and collaborative workflows across sessions."""

#3 Define Project Paths for Data and Results

import os # Import os for path manipulation
project_root = '/content/drive/MyDrive/METABRIC_project'
data_path = '/content/drive/MyDrive/METABRIC_cleaned_imputed.csv' # Corrected path
output_path = os.path.join(project_root, 'outputs')

"""**Step#3** This step defines the directory structure for the project, ensuring that all data inputs and outputs are organized and accessible. It begins by importing the os module to handle path operations. The project_root variable specifies the main folder for the METABRIC project within Google Drive. The data_path points directly to the cleaned and imputed METABRIC dataset, which will be used for analysis. Finally, output_path is constructed by joining the project root with an outputs subfolder, creating a dedicated location for saving results, plots, and model artifacts. This setup promotes reproducibility and keeps the workflow modular and tidy."""

#4 Load METABRIC Dataset and Inspect Schema

import pandas as pd

# Ensure data_path is defined if this cell is run independently
data_path = '/content/drive/MyDrive/METABRIC_cleaned_imputed.csv'

# Load dataset
df = pd.read_csv(data_path)

# Display column names
print("Column names in the dataset:")
print(df.columns)

"""**Step#4** This step initiates the data analysis workflow by loading the METABRIC dataset into a pandas DataFrame. It begins by ensuring the data_path variable is defined, which points to the cleaned and imputed CSV file stored in Google Drive. Using pd.read_csv(data_path), the dataset is read into memory as df. To verify successful loading and understand the dataset's structure, the script prints the column names. These columns span clinical, genomic, and survival-related features, providing a comprehensive foundation for downstream preprocessing and modeling."""

#5 Preview Patient-Level Data Records

display(df.head())

"""**Step#5** This step provides a quick overview of the METABRIC dataset’s structure and contents. By calling df.shape, the code reveals the number of rows and columns, helping assess dataset size and potential computational load. The df.head() function then displays the first few records, offering a snapshot of the data layout, feature types, and value distributions. This preview is crucial for verifying successful loading, identifying formatting issues, and guiding initial preprocessing decisions in clinical modeling workflows."""

#6 Reload METABRIC Dataset for Cell Independence)
data_path = '/content/drive/MyDrive/METABRIC_cleaned_imputed.csv'
df = pd.read_csv(data_path)

"""**Step#6** This step reloads the METABRIC dataset from its designated path into a pandas DataFrame named df. Although similar to Step #4, this reloading ensures that the dataset is freshly instantiated, which is particularly useful when isolating preprocessing or modeling steps in separate notebook cells. It reinforces cell independence—allowing this block to run without relying on prior executions—thus improving modularity and reproducibility in the workflow. This practice is essential in collaborative environments or when debugging segmented pipelines."""

#7 Ensure Output Directory Exists
os.makedirs(output_path, exist_ok=True)

"""**Step#7** This step safeguards the workflow by verifying that the designated output directory is present before saving any results. Using os.makedirs(output_path, exist_ok=True), it programmatically creates the folder specified by output_path if it doesn’t already exist. The exist_ok=True flag prevents errors from being raised if the directory is already there, ensuring smooth execution across reruns. This proactive check is essential for reproducibility and avoids interruptions when writing plots, models, or summary files during analysis."""

#8 Convert Categorical Text to Numeric Codes

categorical_cols = df.select_dtypes(include='object').columns
encoders = {col: LabelEncoder().fit(df[col]) for col in categorical_cols}
for col, encoder in encoders.items():
    df[col] = encoder.transform(df[col])

"""**Step#8** This step transforms all object-type (string-based) categorical columns in the dataset into numeric format using label encoding, a critical preprocessing step for machine learning models that require numerical input. It first identifies all categorical columns via df.select_dtypes(include='object').columns, then constructs a dictionary of fitted LabelEncoder instances for each column. The loop applies each encoder to its respective column, replacing text labels with integer codes. This ensures compatibility with algorithms like Cox regression and facilitates downstream scaling and modeling while preserving feature traceability."""

#9 Standardize Numeric Features for Modeling

numeric_cols = df.select_dtypes(include='number').columns
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

"""**Step#9** This step prepares the numeric features for machine learning by applying standardization, a technique that centers each feature around a mean of 0 and scales it to unit variance. It begins by identifying all numeric columns using df.select_dtypes(include='number').columns, then fits a StandardScaler from scikit-learn to the data. The transformed values replace the original ones in the DataFrame, ensuring that features are on a comparable scale. This is especially important for algorithms sensitive to feature magnitude, such as Cox regression, and helps improve model convergence and interpretability."""

#10 Load Finalized Dataset for Modeling

df = pd.read_csv(os.path.join(output_path, 'METABRIC_preprocessed.csv'))

"""**Step#10** This step retrieves the finalized version of the METABRIC dataset, which has already undergone encoding and scaling, by reading it from a CSV file stored in the designated output directory. Using pd.read_csv(os.path.join(output_path, 'METABRIC_preprocessed.csv')), the code dynamically constructs the file path and loads the data into the df DataFrame. This ensures that subsequent modeling steps operate on a clean, numerically formatted dataset, preserving preprocessing consistency and enabling reproducible analysis across sessions or collaborators."""

#11 Train Cox Model on Cleaned Data

# Drop 'Sex' column due to very low variance which causes convergence issues
df_filtered = df.drop(columns=['Sex'])

cph = CoxPHFitter()
cph.fit(df_filtered, duration_col='Overall Survival (Months)', event_col='Overall Survival Status')

"""**Step#11** This step initiates survival modeling by fitting a Cox Proportional Hazards model using the lifelines library. To ensure model convergence, the Sex column is dropped due to its low variance, which can introduce instability in the optimization process. The filtered dataset df_filtered is then passed to a CoxPHFitter instance, specifying 'Overall Survival (Months)' as the duration column and 'Overall Survival Status' as the event indicator. The model is successfully trained on 2,509 observations with no right-censored cases, setting the stage for interpreting hazard ratios and assessing feature impact on survival outcomes."""

#12 Interpret Cox Model Coefficients and Significance

display(cph.summary)

"""**Step#12** This step presents the statistical output of the fitted Cox Proportional Hazards model, offering insights into how each covariate influences survival outcomes. Using display(cph.summary), the model’s coefficients, hazard ratios (exp(coef)), confidence intervals, standard errors, z-scores, and p-values are tabulated for interpretation. Features with significant p-values and large hazard ratios may indicate strong associations with survival risk. This summary is foundational for clinical inference, helping identify prognostic factors and guiding further feature selection or subgroup analysis in breast cancer research."""

#13 Plot and Save Baseline Survival Curve
cph.baseline_survival_.plot()
plt.title('Baseline Survival Function for Cox Proportional Hazards Model')
plt.xlabel('Time (Months)')
plt.ylabel('Survival Probability')
plt.grid(True)

# Save the plot
plt.savefig(os.path.join(output_path, 'baseline_survival_function_cox_model.png'))
plt.show()

"""**Step#13** This step generates and saves a plot of the baseline survival function derived from the fitted Cox model, offering a visual representation of survival probability over time for the reference group. Using cph.baseline_survival_.plot(), the curve illustrates how the likelihood of survival declines across months, starting at 1.0 and tapering toward zero. The plot is customized with axis labels, a title, and a grid for clarity, then saved to the output directory as a PNG file. This visualization is crucial for understanding the temporal dynamics of survival in the absence of covariate effects."""

#14 Export Cox Model Summary to Text File"

with open(os.path.join(output_path, 'cox_model_summary.txt'), 'w') as f:
    f.write(str(cph.summary))

"""**Step#14** This step ensures that the statistical output of the Cox model is preserved for future reference or reporting by writing the model summary to a text file. Using Python’s with open() syntax, the code constructs the file path with os.path.join(output_path, 'cox_model_summary.txt') and writes the string representation of cph.summary to disk. This practice supports reproducibility, facilitates sharing results with collaborators, and enables downstream automation for documentation or audit trails in clinical modeling workflows."""