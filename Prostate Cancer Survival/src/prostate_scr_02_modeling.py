# -*- coding: utf-8 -*-
"""Prostate_scr/02_modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WtQ_SALM0dU9gUKRmVvd49Dt8Gcva6ce
"""

# 1. Initialize Environment and Mount Drive
import os
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, roc_auc_score, classification_report,
    confusion_matrix
)

from google.colab import drive
drive.mount('/content/drive')

print("Environment ready.")

"""**Step #1** sets up the computational environment needed for the analysis by importing all core Python libraries, machine‑learning modules, and evaluation tools that will be used throughout the workflow. It also mounts Google Drive so the notebook can access datasets, scripts, and output directories stored there. By the end of this step, the environment is fully prepared for data loading, model development, and reproducible experimentation.

**(Why This Step Matters)**
Without this initialization, none of the downstream steps—data loading, preprocessing, model building, or evaluation—would function. Importing libraries upfront prevents interruptions later, and mounting Google Drive ensures that the workflow is reproducible, organized, and capable of interacting with external files. This step establishes the foundation on which the entire pipeline depends.
"""

# 2. Connect to Google Drive for File Access
# ----------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted.")

"""**Step #2** establishes a connection between the Colab notebook and your Google Drive, enabling seamless access to datasets, scripts, and output folders stored there. By mounting Drive, the notebook gains a persistent file system that behaves like a local directory, allowing you to load data directly, save results, and maintain a reproducible workflow across sessions. Once mounted, all Drive‑based resources become available for the remainder of the analysis.

**(Why This Step Matters)**
Google Colab sessions are temporary, and any files stored locally can disappear when the session resets. Mounting Google Drive solves this by providing stable, long‑term storage for your data and outputs. Without this step, you would not be able to reliably load datasets or save trained models, making the workflow fragile and non‑reproducible. This step ensures continuity, organization, and professional‑grade data management.
"""

# 3. Load Preprocessed Dataset
project_root = "/content/drive/MyDrive/prostate_cancer_prediction"
data_path = os.path.join(project_root, "data", "preprocessed",
                         "prostate_cancer_prediction_preprocessed.csv")

try:
    df = pd.read_csv(data_path)
    print("Shape:", df.shape)
except FileNotFoundError:
    print(f"Error: File not found at {data_path}")
    print("Checking directory contents...")
    preprocessed_dir = os.path.join(project_root, "data", "preprocessed")
    data_dir = os.path.join(project_root, "data")
    if os.path.exists(preprocessed_dir):
        print(f"Contents of {preprocessed_dir}:")
        print(os.listdir(preprocessed_dir))
    elif os.path.exists(data_dir):
        print(f"Contents of {data_dir}:")
        print(os.listdir(data_dir))
    else:
        print(f"Neither {preprocessed_dir} nor {data_dir} exist. Please check your project_root and data directory structure.")

"""**Step #3** loads the preprocessed prostate cancer dataset from the project’s directory structure and verifies that the file exists and is accessible. The code constructs the expected file path, attempts to read the CSV into a DataFrame, and prints the dataset’s shape to confirm successful loading. If the file is missing, the step gracefully handles the error by inspecting the relevant directories and listing their contents, helping diagnose issues with file placement or project organization. In this case, the dataset loads successfully with 27,945 rows and 35 columns.

**(Why This Step Matters)**
Every downstream task—exploratory analysis, feature engineering, model training, evaluation—depends on having the correct dataset loaded into memory. A missing or incorrectly structured dataset can silently break the pipeline or produce misleading results. This step ensures reliability by validating both the presence and structure of the data before any modeling begins. It also supports reproducibility by enforcing a clear, organized directory structure.
"""

# 4. Load Dataset and Verify Structure
input_path = data_path # Assign the previously defined data_path to input_path
df = pd.read_csv(input_path)
print("Shape:", df.shape)
df.head()

"""**Step #4** loads the dataset into memory using the file path defined earlier and immediately verifies that the data has been read correctly by printing its shape and previewing the first few rows. This step ensures that the DataFrame is properly structured, contains the expected number of rows and columns, and is ready for downstream analysis. Displaying the head of the dataset provides a quick visual confirmation of feature names, scaling, encoding, and overall data integrity.

**(Why This Step Matters)**
Before performing any modeling, feature engineering, or statistical analysis, you must ensure that the dataset is correctly loaded and structurally sound. A mismatch in row count, missing columns, or corrupted values can silently break the pipeline or lead to invalid results. By validating the dataset immediately after loading, this step prevents downstream errors, supports reproducibility, and ensures that the workflow proceeds with clean, reliable data.
"""

# 5. Define Target Variable and Feature Set
target_col = "Biopsy_Result_Malignant"  # Adjusted target column
assert target_col in df.columns, f"{target_col} not found in columns."

X = df.drop(columns=[target_col])
y = df[target_col]

print("Features shape:", X.shape)
print("Target distribution:")
print(y.value_counts(normalize=True))

"""**Step #5** identifies the target variable for the prediction task and separates it from the feature set. The code first defines the target column (Biopsy_Result_Malignant) and asserts that it exists in the dataset to prevent silent errors. It then constructs the feature matrix X by dropping the target column and assigns the target vector y from that column. Finally, it prints the shape of the feature matrix and displays the normalized distribution of the target classes, providing an immediate check on class balance and dataset structure.

**(Why This Step Matters)**
Machine learning models require a clear distinction between input features and the target variable. If this separation is incorrect or if the target column is missing, the entire modeling pipeline collapses or produces invalid results. Additionally, understanding the class distribution at this stage is critical: imbalanced datasets can bias models, distort accuracy metrics, and require specialized handling. This step ensures structural correctness, prevents subtle errors, and informs modeling decisions.
"""

# 6. Split Data into Training and Test Sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train:", X_train.shape, "Test:", X_test.shape)

"""**Step #6** splits the dataset into training and testing subsets using an 80/20 ratio while preserving the class distribution through stratification. This ensures that both the training and test sets contain proportional representation of the target classes, which is especially important in datasets with imbalance. The code produces four outputs—X_train, X_test, y_train, and y_test—and prints their shapes to confirm that the split occurred correctly. This prepares the data for model training and unbiased performance evaluation.

**(Why This Step Matters)**
A proper train/test split is foundational to building trustworthy machine learning models. Without stratification, an imbalanced dataset could produce a test set that over‑ or under‑represents certain classes, leading to misleading performance metrics. Similarly, without a fixed random state, results would vary between runs, making the workflow non‑reproducible. This step ensures fairness, consistency, and scientific rigor in model evaluation.
"""

# 7. Train Baseline Logistic Regression Model

# Convert target labels to discrete integers (0 and 1)
# Assuming -0.655351 is one class and 1.525900 is the other
label_mapping = {-0.655351: 0, 1.525900: 1}
# Using .replace() for more robust float mapping than .map()
y_train_processed = y_train.replace(label_mapping).astype(int)
y_test_processed = y_test.replace(label_mapping).astype(int)

log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)
log_reg.fit(X_train, y_train_processed)

y_pred_lr = log_reg.predict(X_test)
y_proba_lr = log_reg.predict_proba(X_test)[:, 1]

print("Logistic Regression Accuracy:", accuracy_score(y_test_processed, y_pred_lr))
print("Logistic Regression ROC-AUC:", roc_auc_score(y_test_processed, y_proba_lr))
print("\nClassification Report:\n", classification_report(y_test_processed, y_pred_lr))

"""**Step #7** builds a baseline logistic regression model by first converting the continuous‑looking encoded target values into discrete class labels (0 and 1), ensuring the model receives properly formatted binary targets. After mapping and converting the labels, the logistic regression model is trained on the training data and then used to generate predictions and predicted probabilities on the test set. The step concludes by reporting accuracy, ROC‑AUC, and a full classification report, which reveals that the baseline model predicts only the majority class—highlighting severe class imbalance issues and the need for more advanced modeling strategies.

**(Why This Step Matters)**
A baseline model is essential for understanding how more advanced models should perform. Without this reference point, it’s impossible to know whether later improvements are meaningful or merely cosmetic. Additionally, the poor performance of the baseline model exposes critical issues early—such as label imbalance, feature weakness, or the need for resampling or regularization strategies. This step sets the stage for informed model development and prevents wasted effort on models that appear strong but fail to outperform a simple baseline.
"""

# 8. Train Baseline Random Forest Classifier
rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train_processed)

y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:, 1]

print("Random Forest Accuracy:", accuracy_score(y_test_processed, y_pred_rf))
print("Random Forest ROC-AUC:", roc_auc_score(y_test_processed, y_proba_rf))
print("\nClassification Report:\n", classification_report(y_test_processed, y_pred_rf))

"""**Step #8** trains and evaluates a Random Forest Classifier as a second baseline model to compare against logistic regression. The model is initialized with 200 trees, no depth limit, and parallel processing enabled for efficiency. After fitting the model on the processed training labels, predictions and predicted probabilities are generated for the test set. The step concludes by reporting accuracy, ROC‑AUC, and a full classification report, which—like the logistic regression results—shows that the model predicts only the majority class. This reveals that even a more flexible ensemble method fails to capture meaningful signal under the current data conditions.

**(Why This Step Matters)**
Trying a stronger model early in the workflow helps determine whether the dataset contains enough predictive structure for more advanced algorithms to succeed. When both logistic regression and Random Forest fail in the same way, it signals that the problem lies in the data rather than the model. This insight is crucial: it tells you that the next steps should focus on class imbalance handling, feature engineering, or alternative target definitions rather than simply tuning or swapping models. Step #8 provides a diagnostic checkpoint that shapes the direction of the entire modeling strategy.
"""

# 9. Cross‑Validate Random Forest Model (ROC‑AUC)
cv_scores = cross_val_score(rf, X_train, y_train_processed, cv=5, scoring="roc_auc", n_jobs=-1)
print("CV ROC-AUC scores:", cv_scores)
print("Mean CV ROC-AUC:", cv_scores.mean())

"""**Step #9** evaluates the Random Forest model using 5‑fold cross‑validation on the training set, measuring performance with ROC‑AUC to assess how well the model separates the two classes. By running the model across multiple folds, this step provides a more stable and reliable estimate of model performance than a single train/test split. The resulting scores—hovering around 0.50—indicate that the model performs no better than random guessing, reinforcing earlier findings that the dataset’s current structure, class imbalance, or feature quality may be limiting predictive power.

**(Why This Step Matters)**
Cross‑validation is a critical diagnostic tool: it reveals whether a model’s performance is consistent and whether it generalizes beyond a single train/test split. When both the individual fold scores and the mean ROC‑AUC cluster around 0.50, it signals that the model lacks discriminative power. This insight prevents wasted effort on hyperparameter tuning and instead directs attention toward deeper issues—such as class imbalance, feature engineering, or target definition—that must be addressed before meaningful modeling can occur.
"""

# 10. Save Trained Models to Disk
import joblib

models_path = os.path.join(project_root, "models")
os.makedirs(models_path, exist_ok=True)

joblib.dump(log_reg, os.path.join(models_path, "logistic_regression.pkl"))
joblib.dump(rf, os.path.join(models_path, "random_forest.pkl"))

print("Models saved to:", models_path)

"""**Step #10** saves the trained machine‑learning models both the logistic regression and the random forest into a dedicated models directory within the project structure. Using joblib, the code serializes each model into a .pkl file, ensuring they can be reloaded later without retraining. The step also creates the models folder if it does not already exist, guaranteeing a clean and organized storage location. Once completed, the script prints the path where the models were saved, confirming successful persistence.

**Why This Step Matters)**
Saving trained models is essential for reproducibility, efficiency, and real‑world deployment. Without this step, every future session would require retraining the models wasting time, compute resources, and potentially producing slightly different results due to randomness. Persisting models also enables downstream tasks such as inference pipelines, API integration, model comparison, and versioning. This step transforms the models from temporary in‑memory objects into durable project assets.
"""