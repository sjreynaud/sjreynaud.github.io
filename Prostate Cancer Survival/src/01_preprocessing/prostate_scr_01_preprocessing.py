# -*- coding: utf-8 -*-
"""Prostate_scr/01_preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EZkQqvELDG9N-rQAK4ebHfYac9AgY9Fj
"""

# 1. Connect to Google Drive for File Access
from google.colab import drive
drive.mount('/content/drive')

"""**Step #1** is the foundational setup step where you connect your Google Colab environment to your Google Drive so the notebook can read and write files directly from your Drive. The code imports the drive module from google.colab and mounts your Drive at the path /content/drive, which allows Colab to treat your Drive like a local filesystem. The output message simply confirms that the Drive is already mounted and reminds you that you can force a remount if needed.

**(Why This Step Matters)**
Mounting Google Drive is essential because Colab sessions are temporary—files stored locally in the session disappear when the runtime resets. By mounting Drive, you ensure that your data, intermediate outputs, and final results persist. It also enables consistent directory structures, easier collaboration, and seamless integration with large datasets that would be impractical to upload repeatedly.
"""

# 2. Set paths and load dataset
import pandas as pd
import numpy as np

DATA_PATH = "/content/drive/MyDrive/prostate_cancer_prediction.csv"
OUTPUT_PATH = "/content/drive/MyDrive/prostate_cancer_prediction/prostate_cancer_prediction_cleaned_imputed.csv"

df = pd.read_csv(DATA_PATH)
df.head()

"""**Step 2** is where you formally define the file locations for your workflow and load the raw prostate‑cancer dataset into memory so the rest of the pipeline has a consistent, reproducible starting point. The code sets two paths—one for reading the original CSV and one for saving the cleaned/imputed version—and then uses pandas.read_csv() to load the dataset into a DataFrame. Displaying df.head() confirms that the file was read correctly and gives you a quick visual check of the first few rows, including patient identifiers, clinical features, and diagnostic variables.

**(Why This Step Matters)**
Clear, explicit path definitions prevent errors later in the pipeline, especially in collaborative or multi‑session environments like Colab where directory structures can shift. Loading the dataset early provides a stable foundation for all subsequent steps—cleaning, imputing, feature engineering, and modeling depend on having a correctly loaded DataFrame. This step also supports reproducibility: anyone running your notebook can immediately see where the data lives and confirm that the dataset loads as intended.
"""

# 3. Normalize Column Naming Conventions
df.columns = (
    df.columns
    .str.strip()
    .str.lower()
    .str.replace(" ", "_")
    .str.replace("-", "_")
)
df.columns

"""**Step 3** cleans and standardizes every column name in the dataset so the entire pipeline can reference variables consistently and without risk of typos or formatting mismatches. The code trims whitespace, converts names to lowercase, and replaces spaces and hyphens with underscores, producing a uniform, machine‑friendly naming scheme. The resulting column list shows that all fields—from clinical variables like psa_level and dre_result to lifestyle factors like exercise_regularly—now follow a predictable pattern that supports reliable downstream processing.

**(Why This Step Matters)**
Clean column names are foundational for reproducibility and maintainability. When names follow a predictable pattern, you avoid errors in feature engineering, modeling, and visualization steps that rely on exact string matches. This is especially important in clinical research workflows, where clarity and consistency directly affect the reliability of analyses and the ease with which collaborators can understand and reuse your code. Standardized names also make it easier to automate future steps, such as programmatically selecting feature groups or generating reports.
"""

# 4. Assess Dataset Structure and Data Quality
print(df.info())
print(df.describe(include='all'))
print(df.isna().sum())

"""**Step 4** is the point in your workflow where you take a full diagnostic look at the dataset’s internal structure—its data types, summary statistics, and missing‑value patterns—so you understand exactly what you’re working with before performing any cleaning or modeling. The commands display the dataset’s shape (27,945 rows × 30 columns), show descriptive statistics for both numeric and categorical variables, and list the number of missing values in each column. This reveals that most fields are complete, while a few—such as prostate_volume, genetic_risk_factors, previous_cancer_history, and early_detection—contain missing values that will require attention in later steps.

**(Why This Step Matters)**
Understanding the dataset’s structure early prevents downstream errors and ensures that your cleaning and modeling steps are grounded in the actual characteristics of the data. Missing values must be identified before imputation strategies can be chosen; data types must be correct before encoding or scaling; and summary statistics help you spot anomalies, outliers, or skewed distributions that could bias a predictive model. In clinical research workflows, this structural inspection is essential for reproducibility, transparency, and methodological rigor.
"""

# 5. Impute Missing Values Using Median and Mode
from sklearn.impute import SimpleImputer

continuous_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

# Continuous → median
median_imputer = SimpleImputer(strategy='median')
df[continuous_cols] = median_imputer.fit_transform(df[continuous_cols])

# Categorical → mode
mode_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = mode_imputer.fit_transform(df[categorical_cols])

"""**Step 5** is where the workflow formally resolves all missing values by applying two evidence‑based imputation strategies: median imputation for continuous variables and mode imputation for categorical variables. The code first identifies which columns are numeric and which are categorical, then uses SimpleImputer to fill numeric gaps with the median—protecting the distribution from outliers—and categorical gaps with the most frequent value, preserving the dominant category structure. After this step, the dataset becomes fully complete, consistent, and ready for downstream preprocessing and modeling.

**(Why This Step Matters)**
Machine‑learning models cannot operate on missing values, and improper handling of missingness can distort clinical patterns or introduce bias. Using median and mode imputation preserves the underlying distribution of the data without making strong assumptions about the missing values. This step ensures that the dataset is complete, stable, and analytically sound before feature engineering or model training. In a clinical prediction pipeline, reliable imputation is essential for reproducibility, fairness, and interpretability.
"""

# 6. Deduplicate Records and Enforce Clinical Validity
df = df.drop_duplicates()

# Example clinical sanity checks (customize as needed)
if 'age' in df.columns:
    df = df[(df['age'] >= 18) & (df['age'] <= 100)]

if 'psa' in df.columns:
    df = df[df['psa'] >= 0]

"""**Step 6** cleans the dataset by removing duplicate rows and filtering out clinically impossible or invalid values so that only trustworthy, analysis‑ready records remain. The code first drops any repeated entries, then applies two sanity checks: it keeps only patients with an age between 18 and 100, and it removes any records where PSA values are negative. These checks ensure that the dataset reflects realistic clinical ranges and that downstream modeling is not distorted by erroneous or duplicated data.

**(Why This Step Matters)**
Clinical datasets often contain inconsistencies, duplicates, or out‑of‑range values that can mislead statistical summaries, skew model training, or create false patterns. Removing duplicates protects against artificially inflated sample sizes, while sanity checks enforce biological realism and maintain the integrity of the dataset. This step strengthens the reliability of every subsequent operation—feature engineering, model fitting, and evaluation—by ensuring the data foundation is clean, valid, and clinically coherent.
"""

# 7. Convert Categorical Features to One‑Hot Encoded Variables
df = pd.get_dummies(df, drop_first=True)
df.head()

"""**Step 7** converts all categorical variables into model‑ready numerical features using one‑hot encoding, expanding each category into its own binary indicator column while dropping the first category to prevent multicollinearity. The transformation produces a clean, fully numeric dataset where clinical attributes such as family history, race, DRE results, and biopsy outcomes are represented as interpretable 0/1 features alongside continuous variables like age, PSA level, BMI, and prostate volume. The resulting DataFrame grows to 36 columns, reflecting the richer feature space created by encoding.

**(Why This Step Matters)**
Most machine‑learning algorithms require numerical inputs, and categorical variables must be encoded in a way that preserves information without introducing artificial order or correlation. One‑hot encoding ensures that each category is represented independently, enabling models to learn meaningful patterns from clinical attributes such as biopsy results or family history. This step is especially important in medical prediction pipelines, where categorical features often carry strong diagnostic value and must be encoded cleanly to maintain interpretability and model stability.
"""

# 8. Create Clinically Informed Engineered Features
import numpy as np

# PSA density (if prostate volume exists)
if {'psa', 'prostate_volume'}.issubset(df.columns):
    df['psa_density'] = df['psa'] / df['prostate_volume']

# Age groups
if 'age' in df.columns:
    df['age_group'] = pd.cut(
        df['age'],
        bins=[0, 50, 60, 70, 120],
        labels=['<50', '50-59', '60-69', '70+']
    )
    df = pd.get_dummies(df, columns=['age_group'], drop_first=True)

# Log-transform skewed labs
skewed = ['psa']
for col in skewed:
    if col in df.columns:
        df[f'log_{col}'] = np.log1p(df[col])

"""**Step 8** enriches the dataset with clinically meaningful engineered features by creating PSA density, grouping patients into age categories, and applying a log transformation to skewed variables. PSA density is computed when both PSA and prostate volume are available, producing a more informative ratio than PSA alone. Age is binned into clinically relevant groups and then one‑hot encoded to preserve interpretability while making the feature usable in models. Finally, skewed variables such as PSA are log‑transformed to stabilize variance and reduce the influence of extreme values. Together, these transformations strengthen the dataset by adding domain‑informed predictors and improving statistical behavior.

**(Why This Step Matters)**
Clinical prediction models often benefit from engineered features that reflect real‑world diagnostic reasoning. PSA density is a well‑established clinical marker, age groups align with screening and risk‑stratification guidelines, and log‑transformed biomarkers reduce the dominance of extreme values. These engineered features improve model interpretability, reduce noise, and often boost predictive accuracy. Without this step, the model would rely solely on raw variables, missing important clinical nuance and potentially underperforming.
"""

# 9. Verify Dataset Completeness and Statistical Integrity
print(df.isna().sum())
print(df.describe())

"""**Step 9** performs a final integrity sweep of the dataset by confirming that no missing values remain and by reviewing updated summary statistics after all cleaning, encoding, and feature‑engineering steps. The code prints a full missing‑value count for every column—showing all zeros—and then displays descriptive statistics for key numerical variables such as age, PSA level, BMI, screening age, and prostate volume. This final check verifies that the dataset is complete, numerically stable, and internally consistent before it moves into modeling or analysis.

**(Why This Step Matters)**
A final validation pass is essential before modeling because even small inconsistencies can undermine model performance, bias results, or cause errors during training. Ensuring that the dataset is complete, numerically reasonable, and free of unexpected artifacts protects the integrity of the entire pipeline. In clinical prediction workflows, this step is especially important because downstream interpretations and decisions rely on the reliability of the input data.
"""

# 10. Export Final Preprocessed Dataset
df.to_csv(OUTPUT_PATH, index=False)
print("Saved:", OUTPUT_PATH)

"""**Step 10** finalizes the preprocessing pipeline by writing the fully cleaned, imputed, encoded, and feature‑engineered dataset to a permanent CSV file so it can be reliably used for modeling, sharing, or downstream analysis. The code exports the DataFrame to the predefined OUTPUT_PATH without row indices and prints a confirmation message showing exactly where the file was saved. This step marks the transition from data preparation to modeling, ensuring that the processed dataset is preserved in a stable, reproducible form.

**(Why This Step Matters)**
Saving the dataset is essential for reproducibility, collaboration, and workflow efficiency. It prevents the need to rerun the entire preprocessing pipeline each time you want to train or evaluate a model, and it ensures that all team members or future analyses use the same consistent version of the data. In clinical research, where auditability and version control are critical, having a clearly defined, exported dataset is a foundational requirement for transparent and trustworthy modeling.
"""