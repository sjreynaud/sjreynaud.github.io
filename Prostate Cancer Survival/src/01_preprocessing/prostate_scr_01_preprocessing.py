# -*- coding: utf-8 -*-
"""Prostate_scr/01_preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oXnh9Oc3UIRPsaFLb7301nvpoyqjIS-V
"""

# 1. Initialize Analysis Environment
import os
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from google.colab import drive
drive.mount('/content/drive')

print("Environment ready.")

"""**Step #1** initializes the working environment by importing essential Python libraries for data handling, numerical computation, preprocessing, and file access. It loads pandas and numpy for core data manipulation, brings in StandardScaler and SimpleImputer from scikit‑learn for downstream preprocessing, and mounts Google Drive so the notebook can read and write files stored there. Once everything is loaded and the drive is mounted, the notebook confirms that the environment is ready for analysis.

**(What This Step Does)**
This step prepares the computational workspace by importing the libraries that will be used throughout the analysis and establishing access to external files. Without these imports and the drive mount, none of the later steps—such as loading data, cleaning it, or running models—would function.
"""

# 2. Connect to Google Drive for File Access
# ----------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted.")

"""**Step #2** establishes a connection between your Colab notebook and your Google Drive so the workflow can access datasets, models, and output directories stored there. By importing the google.colab.drive module and mounting the drive to the /content/drive path, this step ensures that all subsequent file operations—loading raw data, saving processed files, exporting figures, or writing model outputs—can be performed seamlessly within the notebook environment. Once mounted, the notebook confirms that Google Drive is successfully accessible.

**(Why This Step Matters)**
This step matters because Colab sessions are temporary, and without mounting Google Drive, you would have no reliable way to access your project files or save your results. Mounting Drive ensures reproducibility, persistence, and smooth integration with your existing folder structure—critical for any structured research workflow.
"""

# 3. Load Processed Dataset

# Define Project File Paths and Output Structure
project_root = "/content/drive/MyDrive/prostate_cancer_prediction"

data_path = os.path.join(
    project_root, "data", "processed", "prostate_cancer_prediction_cleaned_imputed.csv"
)

# Load dataset
df = pd.read_csv(data_path)
print("Shape:", df.shape)
df.head()

"""**Step #3** loads the cleaned and imputed prostate cancer dataset into the notebook so it can be explored, analyzed, and used for modeling. It begins by defining the project’s directory structure, constructs the full path to the processed CSV file, and then reads the dataset into a pandas DataFrame. After loading, the code prints the dataset’s shape—showing 27,945 rows and 30 columns—and displays the first few records to confirm that the data has been successfully imported and is ready for downstream preprocessing and analysis.

**(Why This Step Matters)**
This step matters because every subsequent operation—exploratory analysis, preprocessing, model training, evaluation—depends on having the correct dataset loaded into memory. Verifying the shape and previewing the data early helps catch issues such as incorrect file paths, missing columns, or corrupted files before they propagate into later steps. It establishes a reliable starting point for the analytical workflow.
"""

# 4. Standardize Column Names
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
df.columns

"""**Step #4** standardizes all column names in the dataset to ensure they follow a clean, consistent, and analysis‑friendly format. The code removes any leading or trailing whitespace, converts all column names to lowercase, and replaces spaces with underscores. This produces a uniform naming convention that prevents errors, improves readability, and makes downstream coding—such as feature selection, modeling, and documentation—far more reliable and reproducible.

**(Why This Step Matters)**
This step matters because inconsistent or messy column names can cause subtle bugs, break code, or make the workflow harder for collaborators to understand. Standardized names ensure that every variable can be referenced reliably, support reproducible analysis, and make the dataset easier to document, share, and integrate with modeling libraries that expect clean identifiers.
"""

# 5. Remove duplicate records
before = df.shape[0]
df = df.drop_duplicates()
after = df.shape[0]
print(f"Removed {before - after} duplicate rows.")

"""**Step #5** checks the dataset for duplicate records and removes any that are found to ensure the data is clean, consistent, and free from repeated entries that could distort analysis or model training. The code captures the number of rows before and after applying drop_duplicates(), then reports how many duplicates were removed. In this case, the output shows that no duplicate rows were present, confirming that the dataset is already unique and ready for further preprocessing.

**(Why This Step Matters)**
This step matters because duplicate records can bias statistical summaries, inflate sample size, and mislead machine‑learning models by giving certain patterns undue weight. Removing duplicates ensures data integrity, prevents skewed results, and supports a more reliable and reproducible analysis pipeline.
"""

# 6. Remove Patient Identifiers
id_cols = ["patient_id", "mrn", "record_id"]
df = df.drop(columns=[c for c in id_cols if c in df.columns], errors="ignore")
df.head()

"""**Step #6** the workflow removes all patient‑identifying columns—such as patient_id, mrn, and record_id—from the dataset to ensure that the remaining data contains only analytical features and no personally identifiable information. The code checks whether each identifier column exists and drops it safely using errors="ignore", preventing interruptions if a column is missing. The resulting DataFrame now contains only clinical, demographic, and symptom‑related variables appropriate for downstream modeling and analysis.

**(Why This Step Matters)**
Removing identifiers is essential for patient privacy, regulatory compliance, and ethical data handling. It also prevents machine‑learning models from inadvertently learning patterns tied to individual patients rather than clinical features. Clean, de‑identified data ensures reproducibility, protects confidentiality, and keeps the workflow aligned with HIPAA‑safe practices.
"""

# 7. Standardize Date Columns
date_cols = [c for c in df.columns if "date" in c]
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors="coerce")

df[date_cols].head() if date_cols else "No date columns detected."

"""**Step #7** — This step scans the dataset for any columns whose names contain the substring “date” and attempts to convert them into a standardized datetime format using pd.to_datetime. Each matching column is processed individually, with invalid or unparseable values safely coerced into NaT rather than causing errors. If no date‑related columns exist, the code simply reports that none were detected. This ensures that all temporal fields—when present—are consistently formatted and ready for downstream analysis, feature engineering, or time‑based filtering.

**(Why This Step Matters)**
Consistent datetime formatting is essential for any analysis involving timelines, durations, or event sequencing. Without standardization, date fields can remain as strings, leading to incorrect sorting, failed merges, or inaccurate feature creation. By normalizing date columns early in the workflow, you ensure that the dataset is analytically reliable, reproducible, and ready for any time‑based modeling or reporting.
"""

# 8. Identify Numeric and Categorical Columns
numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns
categorical_cols = df.select_dtypes(include=["object", "category"]).columns

print("Numeric:", len(numeric_cols))
print("Categorical:", len(categorical_cols))

"""**Step #8** — This step automatically identifies which columns in the dataset are numeric and which are categorical by inspecting their underlying data types. Using select_dtypes, the code extracts all columns with numeric types (int64, float64) and all columns with object‑ or category‑based types, storing them in separate lists. The printed counts—5 numeric and 24 categorical—provide a quick snapshot of the dataset’s structure. This classification prepares the dataset for downstream preprocessing steps such as encoding, scaling, imputation, and model‑specific feature handling.

**(Why This Step Matters)**
Machine‑learning models treat numeric and categorical data very differently. Numeric features may require scaling, normalization, or continuous‑value transformations, while categorical features often need encoding, grouping, or frequency analysis. By explicitly identifying these column types early, the workflow becomes more modular, reproducible, and robust—ensuring that each feature receives the correct preprocessing treatment.
"""

# 9. Impute numeric with median
num_imputer = SimpleImputer(strategy="median")
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

"""**Step #9** — This step handles missing values in all numeric columns by applying a median‑based imputation strategy. Using SimpleImputer(strategy="median"), the code calculates the median for each numeric feature and replaces any missing entries with that value. This ensures that the dataset remains complete and numerically stable without introducing extreme or biased values. The transformation is applied directly back to the numeric columns, producing a clean, model‑ready dataset.

**(Why This Step Matters)**
Missing numeric values can break machine‑learning algorithms, distort statistical summaries, and reduce model reliability. Median imputation prevents these issues while avoiding the pitfalls of mean imputation, which can be overly influenced by extreme values. By filling gaps with a stable, representative statistic, this step ensures that downstream models receive complete, consistent, and analytically trustworthy numeric inputs.
"""

# 10. Impute Categorical Columns with Mode
cat_imputer = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

"""**Step #10** — This step addresses missing values in all categorical columns by applying a mode‑based imputation strategy. Using SimpleImputer(strategy="most_frequent"), the code identifies the most common category within each categorical feature and replaces any missing entries with that value. This ensures that categorical variables remain complete, consistent, and usable for downstream encoding or modeling. By applying the transformation directly to the categorical subset of the DataFrame, the dataset becomes cleaner and more reliable without introducing artificial or unrealistic categories.

**(Why This Step Matters)**
Missing categorical values can disrupt encoding steps, reduce model interpretability, and introduce inconsistencies across the dataset. Mode imputation ensures that each categorical feature remains complete while maintaining the natural distribution of categories. This prevents errors during one‑hot encoding or label encoding and supports stable, reproducible modeling. In clinical datasets—where categories often represent symptoms, test results, or patient‑reported information—preserving the original category structure is especially important.
"""

# 11. Create Derived Clinical Features

# BMI from weight/height if present
if {"weight_kg", "height_cm"}.issubset(df.columns):
    df["bmi"] = df["weight_kg"] / (df["height_cm"] / 100) ** 2

# Age from date_of_birth if present
if "date_of_birth" in df.columns:
    df["age"] = (pd.Timestamp("today") - df["date_of_birth"]).dt.days // 365

"""**Step #11** — This step introduces feature engineering by creating new, clinically meaningful variables from existing columns when the necessary data is available. If both weight_kg and height_cm exist, the code computes Body Mass Index (BMI) using the standard formula and adds it as a new feature. Similarly, if date_of_birth is present, the code calculates each patient’s age by subtracting the birth date from today’s date and converting the result into years. These derived features enrich the dataset with additional, model‑useful information that is not explicitly provided in the raw data.

**(Why This Step Matters)**
Feature engineering is one of the most powerful steps in any modeling pipeline because it transforms raw data into more informative, higher‑level variables that can significantly improve predictive performance. BMI and age are clinically relevant predictors in many health‑related models, and deriving them ensures the dataset captures important physiological and demographic information. By generating these features only when the required inputs exist, the pipeline remains flexible, robust, and adaptable across different datasets.
"""

# 12. One‑Hot Encode Categorical Variables
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
df.shape

"""**Step #12** — This step converts all categorical variables into numerical format using one‑hot encoding, a standard technique for preparing categorical data for machine‑learning models. The code applies pd.get_dummies to the DataFrame, generating binary indicator columns for each category while using drop_first=True to avoid multicollinearity by removing the first category in each set. The result is a fully numeric dataset with expanded feature columns, as reflected in the updated shape (27945, 35). This transformation ensures that categorical information is represented in a model‑friendly, mathematically interpretable format.

**(Why This Step Matters)**
Most machine‑learning algorithms cannot directly interpret raw categorical text values. One‑hot encoding translates these categories into a numerical structure that preserves their meaning without imposing an artificial order. Dropping the first category in each set keeps the feature space efficient and avoids linear dependencies that can cause issues in regression‑based models. This step is essential for ensuring that categorical information is accurately and safely incorporated into the modeling pipeline.
"""

# 13. Standardize numeric columns
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

df[numeric_cols].describe().T.head()

"""**Step #13** — This step standardizes all numeric columns using StandardScaler, transforming each feature so that it has a mean of 0 and a standard deviation of 1. The scaler is fit on the numeric subset of the DataFrame and then applied to replace the original values with their standardized counterparts. The resulting summary statistics confirm the transformation: each numeric feature now centers around zero with unit variance. This ensures that all numeric variables contribute proportionally during modeling, especially for algorithms sensitive to feature scale.

**(Why This Step Matters)**
Many machine‑learning algorithms—such as logistic regression, SVMs, neural networks, and K‑means—are highly sensitive to differences in feature scale. Without standardization, variables measured on larger scales can overshadow those on smaller scales, leading to biased or unstable models. Standardizing numeric features ensures fair weighting, improves model convergence, and enhances overall predictive performance. It also supports reproducibility by applying a consistent transformation across datasets.
"""

# 14. Verify Dataset Integrity
print(df.isnull().sum().sort_values(ascending=False).head())
print(df.shape)

"""**Step #14** performs a quick diagnostic check on the dataset to confirm its overall data quality. It calculates the number of missing values in each column, sorts them from highest to lowest, and prints the top results. It then prints the full shape of the dataset—showing the total number of rows and columns. In this case, the output indicates that the key variables listed have zero missing values and that the dataset contains 27,945 rows and 35 columns, confirming that the data is complete and structurally sound.

**(Why This Step Matters)**
Data quality issues—especially missing values—can silently break models, distort results, or cause errors in downstream steps. By verifying that the dataset is complete and that its shape is correct, you reduce the risk of hidden problems and ensure that preprocessing, feature engineering, and modeling steps operate on clean, reliable data. This step acts as a safeguard before more complex operations begin.
"""

# 15. Export Final Processed Dataset
# Define the path for the cleaned data
CLEANED_DATA_PATH = os.path.join(project_root, "data", "processed", "prostate_cancer_prediction_final_processed.csv")
df.to_csv(CLEANED_DATA_PATH, index=False)
print("Saved preprocessed dataset to:", CLEANED_DATA_PATH)

"""**Step #15** finalizes the data preprocessing workflow by exporting the cleaned and fully prepared dataset to a designated file path. The code constructs a clear, organized location within the project directory for storing processed data and then writes the DataFrame to a CSV file without including the index. A confirmation message prints the exact path where the file was saved, ensuring transparency and traceability. This step marks the transition from data preparation to modeling or analysis, providing a stable, reusable dataset for all downstream tasks.

**(Why This Step Matters)**
Exporting the processed dataset is essential for reproducibility and workflow stability. Without saving the cleaned data, you would need to rerun the entire preprocessing pipeline every time you want to train or evaluate a model. Storing the final dataset also ensures consistency across experiments, collaborators, and production environments. It becomes the single source of truth for all subsequent steps.

"""