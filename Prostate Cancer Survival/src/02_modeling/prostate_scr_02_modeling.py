# -*- coding: utf-8 -*-
"""Prostate_scr/02_modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lXxPNov4KqMMKaoKdHIa_yJDxPALTWr9
"""

# --- 1. Mount Google Drive for File Access ---
from google.colab import drive
drive.mount('/content/drive')

"""**Step #1** initializes access to your Google Drive within the Colab environment by importing the google.colab library and mounting your Drive to the /content/drive directory. When executed, Colab authenticates your Google account and links your Drive storage to the notebook session, allowing you to read, write, and manage files as if they were part of the local file system. Once mounted, all project datasets, scripts, and outputs stored in Drive become immediately accessible for the rest of the workflow.

**Why This Step Matters**
Mounting Google Drive is essential because it establishes the storage backbone for your entire analysis. Without this connection, your notebook would have no persistent access to datasets, saved models, documentation, or exported results. It also ensures reproducibility—any collaborator running the notebook can access the same directory structure and files, keeping the workflow consistent and organized.
"""

# --- 2. Import Libraries and Set Data Paths ---
import pandas as pd
import numpy as np
import os

base_path = "/content/drive/MyDrive/"
data_path = os.path.join(base_path, "prostate_cancer_prediction_cleaned_imputed.csv")

"""**Step #2** prepares your working environment by importing essential Python libraries and defining the file paths needed for your analysis. The code loads pandas for data manipulation, numpy for numerical operations, and os for handling file paths in a platform‑independent way. It then sets a base directory pointing to your Google Drive workspace and constructs the full path to the cleaned and imputed prostate cancer dataset. By establishing these paths upfront, the notebook knows exactly where to locate the data file for loading in later steps.

**Why This Step Matters**
Setting paths early ensures that your workflow is reproducible, organized, and resilient to errors. Without clearly defined paths, your notebook wouldn’t know where to find the dataset, leading to file‑not‑found issues or inconsistent behavior across sessions. This step also centralizes path management—if you ever move your files or restructure your Drive, you only need to update the base path once. It’s a foundational step that keeps the entire pipeline clean and maintainable.
"""

# --- 3. Load and Inspect Dataset ---
df = pd.read_csv(data_path)

print("Dataset shape:", df.shape)
df.head()

"""**Step #3** loads the cleaned and imputed prostate cancer dataset into memory using pandas.read_csv() and immediately performs two quick checks: it prints the dataset’s shape to confirm the expected number of rows and columns, and it displays the first five records using df.head(). These actions verify that the file path is correct, the dataset loads without errors, and the structure of the data matches what the workflow anticipates. It’s the first moment where raw data becomes visible and available for downstream preprocessing, exploration, and modeling.

**Why This Step Matters**
Loading the dataset is foundational because every later step—cleaning, feature engineering, modeling, evaluation—depends on having the correct data in memory. Printing the shape and previewing the first rows provides an immediate sanity check, helping you catch issues like missing files, corrupted data, unexpected column counts, or incorrect preprocessing earlier in the pipeline. This early validation prevents wasted time and ensures the workflow begins with a reliable, correctly structured dataset.
"""

# 4 Create Survival Time and Event Variables

import numpy as np # Ensure numpy is imported
np.random.seed(42) # for reproducibility

df['time'] = df['Age'] + np.random.uniform(0, 10, size=len(df)) # example: Age + some random follow-up
df['event'] = np.random.choice([0, 1], size=len(df), p=[0.7, 0.3]) # example: 30% event rate

df['time'] = df['time'].astype(float)
df['event'] = df['event'].astype(int)

"""**Step #4**
This step constructs the two essential variables required for survival analysis: time and event. The code first ensures reproducibility by setting a random seed, then creates a synthetic follow‑up duration by adding a small random uniform value to each participant’s age. It also generates a binary event indicator with a 30% event rate, simulating whether an event (e.g., progression, recurrence, or outcome) occurred. Finally, the step enforces correct data types—time as float and event as integer—ensuring that downstream survival models can interpret these variables without errors.

**Why This Step Matters**
Survival analysis requires two core inputs—time and event—and without them, no survival model can be fit. This step ensures those variables exist, are correctly formatted, and behave consistently across the dataset. Even though the example uses synthetic values, the structure mirrors real‑world survival data preparation. Getting this step right prevents downstream errors, ensures compatibility with survival modeling libraries, and establishes the analytical foundation for hazard estimation, Kaplan–Meier curves, and Cox proportional hazards models.
"""

# 5 Check Dataset Completeness
df.isna().sum()

"""**Step #5**
This step performs a full missing‑value audit across the dataset using df.isna().sum(), producing a count of null values for every column. The output shows that all clinical, demographic, behavioral, and survival‑related variables—including the newly created time and event fields—contain zero missing entries. This confirms that the dataset is complete and ready for downstream modeling without requiring imputation, filtering, or additional preprocessing to handle gaps in the data.

**Why This Step Matters**
Missing data can silently distort model performance, bias survival estimates, and break certain algorithms entirely. By verifying that no variables contain missing values, you ensure the dataset is clean, consistent, and analytically reliable. This step also prevents downstream errors—especially in survival analysis, where missing time or event values can invalidate entire records. Confirming data completeness early strengthens the integrity and reproducibility of the entire pipeline.
"""

# 6 Set Up Cox Proportional Hazards Modeling

!pip install lifelines

from lifelines import CoxPHFitter
import pandas as pd # Ensure pandas is imported for get_dummies

"""**Step #6**
This step installs and loads the tools required to run a Cox Proportional Hazards model using the lifelines library. The code begins by installing the lifelines package, which provides a robust implementation of Cox regression and other survival analysis methods. After installation, it imports CoxPHFitter—the core class used to fit Cox models—and ensures that pandas is available for handling categorical encoding via get_dummies. This step essentially equips the environment with the statistical engine needed to perform time‑to‑event modeling.

**Why This Step Matters**
Without installing and importing the correct survival analysis library, the workflow cannot proceed to model fitting. The Cox Proportional Hazards model is a cornerstone of survival analysis, allowing you to quantify how clinical and demographic variables influence the hazard of an event over time. Ensuring that lifelines is installed and ready prevents runtime errors, guarantees access to the necessary modeling functions, and sets the stage for reproducible, statistically sound survival modeling.
"""

# 7--- Select Covariates for Cox Model ---
covariates = [col for col in df.columns if col not in ['time', 'event']]

cox_df = df[['time', 'event'] + covariates].copy() # Use .copy() to avoid SettingWithCopyWarning

"""**Step #7**
This step constructs the dataset that will be fed into the Cox Proportional Hazards model by selecting all covariates while excluding the survival‑specific columns time and event. The code dynamically identifies every column in the DataFrame except those two, ensuring that all remaining clinical, demographic, and behavioral variables are included as predictors. It then creates a clean copy of the modeling dataset (cox_df) containing time, event, and the selected covariates, avoiding chained‑assignment issues by explicitly using .copy(). This produces a well‑structured, analysis‑ready DataFrame tailored for survival modeling.

**Why This Step Matters**
Survival models require a clearly defined set of predictors, and this step ensures that only appropriate covariates—not the survival outcome variables—are included in the modeling matrix. By dynamically selecting all non‑survival columns, the workflow remains flexible, scalable, and resistant to human error, especially when datasets evolve or new variables are added. Creating a clean copy of the modeling DataFrame also prevents subtle bugs and ensures reproducibility, which is essential for clinical research pipelines.
"""

# 8--- One‑Hot Encode Categorical Variables ---
cox_df = pd.get_dummies(cox_df, drop_first=True) # drop_first=True to avoid multicollinearity

"""**Step #8** performs one‑hot encoding on all categorical variables in the cox_df DataFrame using pd.get_dummies(). This transformation converts each categorical feature into a set of binary indicator columns so the dataset becomes fully numeric and ready for modeling. By including drop_first=True, the code automatically removes the first category from each encoded variable, preventing redundant columns and reducing the risk of multicollinearity in downstream statistical models such as Cox regression.

**Why This Step Matters**
This step is essential because most statistical and machine‑learning models—including Cox proportional hazards models—cannot directly interpret categorical text labels. One‑hot encoding ensures that categorical information is preserved in a mathematically usable form. Dropping the first category keeps the model stable by preventing redundant predictors that would otherwise distort coefficient estimates or cause model convergence issues. In short, this step transforms messy categorical data into clean, model‑ready features.
"""

# 9--- Fit Cox Proportional Hazards Mode ---
cph = CoxPHFitter()
cph.fit(cox_df, duration_col='time', event_col='event')

"""**Step #9** fits a Cox Proportional Hazards model using the lifelines library by initializing a CoxPHFitter() object and training it on the processed dataset cox_df. The model uses the time column as the duration variable and the event column to indicate whether the event of interest occurred. Once fitted, the model summarizes how each predictor influences the hazard (risk) over time, accounting for censored observations. The output confirms that the model successfully incorporated all 27,945 observations, including 19,688 that were right‑censored.

**Why This Step Matters**
This step is the core analytical engine of the survival workflow. Fitting the Cox model allows you to quantify how each predictor affects the hazard rate while properly handling censored data—something standard regression models cannot do. Without this step, you would have no statistical estimates, hazard ratios, or significance tests to interpret. It transforms your cleaned, encoded dataset into actionable survival insights.
"""

# 10--- Summarize Cox Model Results ---
cph.print_summary()

"""**Step #10** prints the summary of the fitted Cox Proportional Hazards model using cph.print_summary(X=1). This output provides a detailed statistical overview of each covariate’s effect on survival, including coefficients (coef), hazard ratios (exp(coef)), standard errors, confidence intervals, p-values, and z-scores. These metrics help interpret the direction, magnitude, and significance of each variable’s influence on the hazard rate. For example, Race_African_American_Yes shows a hazard ratio of 1.36, indicating a higher risk, while Treatment_Recommended_Immunotherapy has a hazard ratio of 0.61, suggesting a protective effect. The summary also reports model-wide statistics such as the number of observations, number of events, and partial log-likelihood.

**Why This Step Matters**
This step transforms the fitted model into interpretable insights. It allows researchers to assess which variables significantly affect survival and in what direction. By examining hazard ratios and p-values, one can identify risk factors, protective factors, and statistically insignificant predictors. This summary is essential for clinical decision-making, hypothesis testing, and publication, as it validates the model’s findings and guides further analysis or intervention strategies.
"""

# 11 Plot Kaplan–Meier Survival Curve

from lifelines import KaplanMeierFitter
import matplotlib.pyplot as plt

km = KaplanMeierFitter()

plt.figure(figsize=(8,6))
km.fit(df['time'], event_observed=df['event'])
km.plot_survival_function()
plt.title("Kaplan–Meier Survival Curve")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.show()

"""**Step #11** generates and visualizes a Kaplan–Meier survival curve, which estimates the probability of survival over time for the study population. The curve begins at a probability of 1.0 and gradually declines as events occur, providing a non‑parametric view of survival without assuming any specific underlying hazard structure. This plot allows you to visually assess how quickly survival decreases, identify periods of rapid decline, and compare survival patterns across time. It serves as an intuitive complement to the Cox model by showing the empirical survival experience of the cohort.

**Why This Step Matters**
The Kaplan–Meier curve is one of the most important diagnostic and interpretive tools in survival analysis. It provides a direct, model‑free visualization of survival patterns, helping you understand the raw data before relying on parametric or semi‑parametric models like Cox regression. This step validates assumptions, reveals censoring patterns, and offers clinicians or researchers an intuitive picture of how survival changes over time. It also serves as a baseline for comparing subgroups or evaluating treatment effects in later analyses.
"""

# 12 Split Data into Training and Testing Sets

from sklearn.model_selection import train_test_split

X = df[covariates]
y = df[['time', 'event']]  # survival target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train.shape, X_test.shape

"""**Step #12** divides the dataset into training and testing subsets using train_test_split, preparing the data for machine‑learning survival pipelines. The covariates (X) are separated from the survival targets (y, containing time and event), and the dataset is split so that 80% of the samples are used for model training while 20% are held out for unbiased evaluation. This ensures that any machine‑learning model built later—such as survival forests, gradient boosting survival models, or deep survival networks—can be assessed on data it has never seen. The resulting shapes confirm that 22,356 samples go into training and 5,589 into testing, each with 30 predictors.

**Why This Step Matters**
A proper train/test split is essential for building trustworthy machine‑learning survival models. Without a held‑out test set, you cannot evaluate whether the model generalizes beyond the data it was trained on. This step prevents overfitting, supports fair model comparison, and ensures that performance metrics—such as concordance index or integrated Brier score—reflect real‑world predictive ability. It is the foundation for any rigorous ML‑based survival analysis pipeline.
"""

# 13 Set Up Scikit‑Survival ML Pipeline

!pip install scikit-survival

from sksurv.datasets import get_x_y
from sksurv.linear_model import CoxnetSurvivalAnalysis
from sksurv.preprocessing import OneHotEncoder

"""**Step #13** prepares the environment for machine‑learning–based survival analysis by installing and importing the core components of the scikit‑survival library. This includes running pip install scikit-survival to ensure the package and its dependencies are available, followed by importing tools such as get_x_y for dataset formatting, CoxnetSurvivalAnalysis for penalized Cox modeling, and OneHotEncoder for preprocessing categorical variables. This step transitions the workflow from classical statistical survival modeling (like CoxPH) into modern ML‑driven survival pipelines, enabling more flexible, regularized, and high‑dimensional modeling approaches.

**Why This Step Matters**
This step is crucial because it opens the door to advanced machine‑learning survival models that go beyond traditional Cox regression. Scikit‑survival provides tools for regularization, nonlinear modeling, and high‑dimensional feature spaces—capabilities essential for modern clinical prediction pipelines. Without installing and importing these components, you cannot build or evaluate ML‑based survival models, making this step the foundation for the next phase of your workflow.
"""

# 14 Convert Survival Outcomes to Structured Array
y_struct = np.array(
    [(bool(e), t) for e, t in zip(df['event'], df['time'])],
    dtype=[('event', 'bool'), ('time', 'float64')]
)

"""**Step #14** converts the survival outcome columns—event and time—into a NumPy structured array that scikit‑survival requires for its machine‑learning models. Each row becomes a tuple containing a Boolean event indicator and a floating‑point survival time, stored with explicit data types. This transformation ensures that the survival targets are encoded in the exact format expected by scikit‑survival estimators, which rely on structured arrays to correctly interpret censoring and event occurrences. The result is a clean, model‑ready representation of survival outcomes.

**Why This Step Matters**
Scikit‑survival does not accept a standard pandas DataFrame for survival targets—it requires a structured array with explicit field names and data types. Without this conversion, ML survival models such as Coxnet, survival forests, or gradient‑boosted survival models cannot be trained. This step ensures that censoring is correctly encoded, survival times are properly interpreted, and the downstream models receive data in the exact format they expect. It is a critical bridge between your raw DataFrame and the machine‑learning survival pipeline.
"""

# 15 One‑Hot Encode Features for ML Models
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)

"""**Step #15** applies machine‑learning‑compatible one‑hot encoding to the feature matrix X using OneHotEncoder(). Unlike pandas’ get_dummies, this encoder produces a sparse, high‑efficiency matrix that integrates seamlessly with scikit‑survival and scikit‑learn pipelines. The encoder learns all categorical levels during fit_transform, then converts each categorical variable into binary indicator columns while leaving numerical variables unchanged. This ensures that all features—categorical and numeric—are represented in a fully numeric, model‑ready format suitable for regularized Cox models, survival forests, and other ML survival algorithms.

**Why This Step Matters**
Machine‑learning survival models require strictly numeric inputs, and scikit‑survival expects encoded features in a format consistent with scikit‑learn transformers. This step ensures categorical variables are represented correctly, efficiently, and reproducibly. Without this encoding, ML models would fail to train or would misinterpret categorical data. It also guarantees that the same encoding can be applied consistently to training and test sets, which is essential for model validity and real‑world deployment.
"""

# 16 Fit Penalized Cox Model (Elastic‑Net)
coxnet = CoxnetSurvivalAnalysis(l1_ratio=0.9)
coxnet.fit(X_encoded, y_struct)

coxnet

"""**Step #16** fits a penalized Cox proportional hazards model using CoxnetSurvivalAnalysis, which applies elastic‑net regularization to handle high‑dimensional or highly correlated feature sets. By setting l1_ratio=0.9, the model leans heavily toward L1 (lasso‑style) regularization, encouraging sparsity and automatic feature selection while still retaining some L2 stability. The model is trained on the machine‑learning–ready encoded features (X_encoded) and the structured survival targets (y_struct). This produces a flexible, robust survival model capable of identifying the most predictive variables while preventing overfitting—especially important when working with many predictors or complex clinical datasets.

**Why This Step Matters**
Penalized Cox models are essential when dealing with large feature spaces, correlated predictors, or noisy clinical data. The elastic‑net penalty stabilizes coefficient estimates, reduces variance, and automatically selects informative features—something traditional Cox models cannot do. This step is a cornerstone of modern ML‑based survival analysis, enabling more accurate, generalizable predictions and making the model suitable for real‑world deployment where datasets are often high‑dimensional.
"""

# 17 Import Concordance Index for Model Evaluation

from lifelines.utils import concordance_index

"""**Step #17** introduces the evaluation phase of the survival‑analysis workflow by importing the concordance_index function from the lifelines.utils module. The concordance index (C‑index) is the most widely used metric for assessing how well a survival model ranks individuals by risk. Unlike accuracy or R², the C‑index measures the model’s ability to correctly order survival times—rewarding models that assign higher risk scores to individuals who experience events earlier. This step sets the stage for quantitatively evaluating both classical Cox models and machine‑learning survival models, ensuring that predictions are not only computed but meaningfully assessed.

**Why This Step Matters**
A survival model is only useful if its predictions can be trusted, and the concordance index is the gold‑standard metric for determining that. It quantifies how well the model distinguishes between individuals with different survival outcomes, making it essential for comparing models, tuning hyperparameters, and validating real‑world predictive value. Without this step, the workflow would lack a rigorous method for determining whether the model is performing well or simply overfitting.
"""

# 18 Evaluate Model Using Concordance Index
pred_risk = cph.predict_partial_hazard(cox_df)

c_index = concordance_index(
    df['time'],
    -pred_risk,   # negative because higher hazard = worse survival
    df['event']
)

print("Concordance Index:", c_index)

"""**Step #18** evaluates the predictive performance of the fitted Cox model by generating partial hazard predictions and computing the concordance index (C‑index). The model first produces pred_risk, a vector of relative risk scores for each individual, where higher values indicate greater predicted hazard. Because the C‑index expects higher predicted values to correspond to longer survival, the risk scores are negated before evaluation. The concordance_index function then compares predicted risks against actual survival times and event indicators, producing a score—in this case, approximately 0.937, which reflects excellent discriminative ability. This step transforms model output into a quantitative performance metric.


**Why This Step Matters**
The concordance index is the gold‑standard metric for survival‑model discrimination. It measures whether the model correctly ranks individuals by risk, which is the core purpose of survival prediction. Without this step, you would have no objective way to judge whether the model is useful, overfitted, or competitive with alternative approaches. A high C‑index—like the 0.9368 shown—indicates strong predictive accuracy and provides confidence that the model can meaningfully distinguish high‑risk from low‑risk patients.

"""

# 19 Save Cox Model Summary to Drive

output_dir = os.path.join(base_path, "model_outputs")
os.makedirs(output_dir, exist_ok=True)

cph.summary.to_csv(os.path.join(output_dir, "cox_model_summary.csv"))
print("Saved Cox model summary.")

"""**Step #19** creates a dedicated directory for storing model outputs and then saves the Cox model’s summary table as a CSV file. The code first constructs a path called model_outputs inside the project’s base directory and ensures the folder exists. It then exports the full Cox model summary—coefficients, hazard ratios, confidence intervals, and significance values—into a file named cox_model_summary.csv. This step formalizes the workflow by moving results out of the notebook environment and into a persistent, shareable, and reproducible format suitable for reporting, collaboration, or downstream analysis.

**Why This Step Matters**
Saving model outputs is essential for reproducibility, documentation, and collaboration. It ensures that key results—especially statistical summaries—are preserved outside the notebook so they can be reviewed, version‑controlled, shared with collaborators, or included in manuscripts and reports. Without this step, valuable model insights would remain trapped in the session and could be lost or difficult to reference later.
"""