# -*- coding: utf-8 -*-
"""Prostate_scr/01_preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJKvxQ9enNP5LAUbagfNsaDcHdJmPfWt
"""

# ============================================================
# 1. Initialize Analysis Environment and Core Libraries
# ============================================================

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

print("Environment initialized.")

"""**Step #1** sets up the core analysis environment by importing all foundational Python libraries needed for data handling, numerical computation, visualization, and preprocessing. This includes pandas and numpy for data manipulation, seaborn and matplotlib for plotting, and scikit‑learn tools such as LabelEncoder, StandardScaler, and SimpleImputer for encoding, scaling, and handling missing values. The step concludes with a confirmation message indicating that the environment is ready, ensuring the workflow begins with a consistent and fully loaded toolset.

**Why This Step Matters**
Initializing the environment upfront creates a stable foundation for reproducible research. It guarantees that the analysis begins with a consistent set of dependencies, which is crucial for debugging, collaboration, and long‑term maintainability. When others read or run the notebook, they immediately understand which libraries the project relies on, making the workflow transparent and easier to extend.
"""

# ============================================================
# 2. Connect to Google Drive for File Access
# ============================================================

from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted.")

"""**Step #2** establishes a connection between the Colab environment and Google Drive, enabling seamless access to project files, datasets, models, and outputs stored in Drive. By mounting the Drive directory, the notebook gains a persistent file path (/content/drive) that allows reading, writing, and organizing files as if they were part of the local environment. This ensures that all data resources remain accessible across sessions and that outputs generated during analysis can be saved directly back to Drive for long‑term storage and reproducibility.

**Why This Step Matters**
Mounting Google Drive is essential for workflows that rely on external datasets, shared resources, or persistent storage. Without this connection, files would be limited to Colab’s temporary runtime, which resets frequently and cannot retain outputs. By linking to Drive, the workflow becomes stable, collaborative, and reproducible—critical qualities for clinical research pipelines where data integrity and traceability matter.
"""

# ============================================================
# 3. Define Project File Paths and Output Structure
# ============================================================

project_root = '/content/drive/MyDrive/prostate_cancer_prediction'

data_path = os.path.join(
    project_root,
    "data",
    "processed",
    "prostate_cancer_prediction_cleaned_imputed.csv"
)

output_path = os.path.join(project_root, "data", "preprocessed")
CLEANED_DATA_PATH = os.path.join(
    output_path,
    "prostate_cancer_prediction_preprocessed.csv"
)

os.makedirs(output_path, exist_ok=True)

print("Project paths set.")
print("Data path:", data_path)
print("Output path:", output_path)
print("Cleaned data save path:", CLEANED_DATA_PATH)

"""**Step #3** establishes the directory structure and file paths that the workflow will use throughout the prostate cancer prediction project. It defines the project’s root folder, identifies the location of the cleaned/imputed input dataset, and sets the output directory where preprocessed data will be saved. The code also ensures that the output directory exists by creating it if necessary, preventing file‑write errors later in the pipeline. Finally, it prints each path to confirm that the environment is correctly configured, giving the user full visibility into where data is coming from and where new files will be stored.

**Why This Step Matters**
Clear, consistent file paths are essential for reproducible research. By defining all directories and filenames at the start, the workflow avoids hard‑coded paths scattered throughout the notebook, reduces the risk of saving files to the wrong location, and ensures that collaborators can easily understand and adapt the project structure. This step also enforces good data hygiene—keeping raw, processed, and preprocessed files organized—making the entire pipeline more maintainable and transparent.
"""

# ============================================================
# 4. Load and Inspect Raw Dataset
# ============================================================

df = pd.read_csv(data_path)

print("Dataset loaded successfully.")
print("Shape:", df.shape)
print("Columns:", list(df.columns))

"""**Step #4** loads the raw prostate cancer dataset from the previously defined file path and performs an initial inspection to confirm that the data has been imported correctly. After reading the CSV into a pandas DataFrame, the step prints a confirmation message, displays the dataset’s shape, and lists all column names. This quick structural overview helps verify that the dataset contains the expected number of rows and features, ensuring that the workflow begins with a clear understanding of the data’s dimensions and available variables before any cleaning or preprocessing occurs.

**Why This Step Matters**
Before any preprocessing or modeling can begin, it’s essential to confirm that the dataset has been loaded correctly and that its structure matches expectations. Inspecting the shape and column names helps catch issues early—such as missing files, incorrect paths, unexpected formatting changes, or incomplete data. This step establishes a baseline understanding of the dataset and ensures that all subsequent steps operate on verified, correctly structured input.
"""

# ============================================================
# 5. Standardize Column Names
# ============================================================

df.columns = (
    df.columns
    .str.strip()
    .str.lower()
    .str.replace(" ", "_")
)

print("Column names standardized.")

"""**Step #5** standardizes all column names in the dataset to ensure they follow a clean, consistent, and machine‑friendly format. The code removes leading and trailing whitespace, converts all characters to lowercase, and replaces spaces with underscores. This normalization step prevents issues that arise from inconsistent naming conventions—such as errors during merging, feature selection, or modeling—and creates a predictable structure that simplifies downstream processing and improves reproducibility.

**Why This Step Matters**
Consistent column naming is essential for a stable and maintainable data pipeline. Many preprocessing and modeling functions rely on exact column name matching, and even small inconsistencies—extra spaces, capitalization differences, or embedded spaces—can cause silent failures or hard‑to‑trace bugs. By enforcing a uniform naming convention early in the workflow, you reduce friction, improve readability, and ensure that all subsequent steps operate on a clean, predictable dataset.
"""

# ============================================================
# 6. Remove Duplicate Records
# ============================================================

df = df.drop_duplicates()
print("Duplicates removed. New shape:", df.shape)

"""**Step #6** removes duplicate records from the dataset to ensure that each patient entry appears only once and that no repeated rows distort the analysis. Using df.drop_duplicates(), the step filters out any exact duplicate rows and updates the DataFrame accordingly. After the operation, the code prints the new shape of the dataset, confirming how many rows remain and verifying that the dataset now contains only unique observations. This creates a cleaner, more reliable foundation for downstream preprocessing and modeling.

**Why This Step Matters**
Duplicate entries can artificially inflate sample size, bias statistical summaries, distort model training, and lead to misleading conclusions—especially in clinical datasets where each row represents a patient or clinical event. Removing duplicates ensures data integrity, prevents overrepresentation of repeated cases, and supports accurate, reproducible modeling. This step is a fundamental quality‑control measure that protects the validity of the entire pipeline.
"""

# ============================================================
# 7. Remove Identifier Columns for De‑Identification
# ============================================================

id_cols = ["patient_id", "mrn", "record_id"]

df = df.drop(columns=[col for col in id_cols if col in df.columns], errors="ignore")

print("Identifier columns removed (if present).")

"""**Step #7** removes personally identifiable columns from the dataset to ensure proper de‑identification before further analysis. The code defines a list of potential identifier fields—such as patient_id, mrn, and record_id—and then drops any of these columns that are present in the DataFrame. By using a conditional list comprehension and errors="ignore", the step safely removes identifiers without causing interruptions if some columns are missing. This ensures that the dataset retains only analytical features while protecting patient privacy and maintaining compliance with data‑handling standards.

**Why This Step Matters**
Removing identifier columns is essential in clinical research pipelines because it protects patient privacy and ensures compliance with HIPAA, IRB requirements, and ethical data‑handling practices. Identifiers can unintentionally leak sensitive information or allow re‑identification when combined with other variables. By stripping these fields early in the workflow, you create a safer dataset for analysis, sharing, and modeling, while maintaining the integrity and confidentiality of patient records.
"""

# ============================================================
# 8. Standardize and Convert Date Columns
# ============================================================

date_cols = [col for col in df.columns if "date" in col]

for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors="coerce")

print("Date columns standardized:", date_cols)

"""**Step #8** identifies any columns in the dataset that contain the substring "date" in their names and standardizes them by converting their values into a proper datetime format. Using pd.to_datetime with errors="coerce", the step ensures that valid dates are parsed correctly while invalid or inconsistent entries are safely converted to NaT. After processing, the step prints the list of detected date columns, confirming which fields were standardized. Even if no date columns are present—as in this case—the step ensures the pipeline is robust and ready for datasets that may include temporal features.

**Why This Step Matters**
Date fields often arrive in inconsistent formats, making them difficult to analyze or use in modeling without standardization. Converting them to a uniform datetime type enables reliable feature engineering—such as extracting year, month, or time intervals—and prevents errors in downstream steps that expect properly formatted dates. Even when no date columns exist, including this step ensures the pipeline remains generalizable, reusable, and resilient across different datasets and future updates.
"""

# ============================================================
# 9. Identify Numeric and Categorical Columns
# ============================================================

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

print("Numeric columns:", list(numeric_cols))
print("Categorical columns:", list(categorical_cols))

"""**Step #9** automatically identifies which columns in the dataset are numeric and which are categorical, allowing the pipeline to treat each type appropriately during preprocessing and modeling. Using df.select_dtypes, the code extracts all columns with numeric data types (int64, float64) and all columns with object‑ or category‑based types. It then prints both lists, providing a clear overview of the dataset’s structure and ensuring that downstream steps—such as imputation, encoding, scaling, and model training—can apply the correct transformations to each variable type.

**Why This Step Matters**
Different types of data require different preprocessing strategies: numeric features may need scaling or imputation with statistical values, while categorical features often require encoding or grouping. Misclassifying a column can lead to incorrect transformations, model errors, or distorted results. By explicitly identifying numeric and categorical columns early in the pipeline, you create a reliable foundation for all subsequent preprocessing steps and ensure that each feature receives the correct treatment.
"""

# ============================================================
# 10. Impute Missing Values (Median for Numeric, Mode for Categorical)
# ============================================================

num_imputer = SimpleImputer(strategy="median")
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

cat_imputer = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

print("Missing values imputed.")

"""**Step #10** handles missing data by applying targeted imputation strategies to both numeric and categorical features. Numeric columns are imputed using the median, a robust measure that reduces the influence of outliers, while categorical columns are imputed using the most frequent value (mode), preserving the most common category in each feature. The step uses SimpleImputer to fit and transform each group of columns, ensuring that all missing values are replaced consistently and systematically. Once complete, the dataset becomes fully populated and ready for downstream preprocessing and modeling.

**Why This Step Matters**
Missing values can break preprocessing pipelines, distort statistical summaries, and cause machine‑learning models to fail or behave unpredictably. Choosing the right imputation strategy is essential: the median protects numeric features from skew caused by extreme values, while the mode preserves the dominant category in categorical features. By resolving missingness early and consistently, this step strengthens data integrity, ensures model compatibility, and prevents subtle biases that could compromise clinical insights.
"""

# ============================================================
# 11. Feature Engineering and Transformations
# ============================================================

# Example: BMI (if available)
if {"weight_kg", "height_cm"}.issubset(df.columns):
    df["bmi"] = df["weight_kg"] / (df["height_cm"] / 100)**2
    print("BMI feature engineered.")

# Example: Age from DOB
if "date_of_birth" in df.columns:
    df["age"] = (pd.Timestamp("today") - df["date_of_birth"]).dt.days // 365
    print("Age feature engineered.")

# One-hot encode categorical variables
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
print("Categorical variables one-hot encoded.")

# Standardize numeric columns
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print("Numeric features standardized.")

"""**Step #11** performs a series of feature engineering and transformation tasks to prepare the dataset for modeling. It begins by creating new derived features when the necessary raw inputs are available—for example, calculating BMI from weight and height or computing age from a date‑of‑birth field. Next, it applies one‑hot encoding to all categorical variables, converting them into machine‑readable binary indicators while avoiding multicollinearity by dropping the first category. Finally, it standardizes all numeric features using StandardScaler, ensuring that each numeric variable has a consistent scale and distribution. Together, these transformations enhance the dataset’s predictive power and make it suitable for a wide range of machine‑learning algorithms.

**Why This Step Matters**
Feature engineering is one of the most impactful stages in any machine‑learning pipeline. Derived features like BMI or age can capture clinically meaningful patterns that raw variables alone may not reveal. Encoding categorical variables ensures that models can interpret them correctly, and standardizing numeric features prevents algorithms from being biased toward variables with larger scales. By applying these transformations systematically, the pipeline improves model stability, enhances predictive performance, and ensures that all features contribute appropriately to the learning process.
"""

# ============================================================
# 12. Validate Data Quality and Final Structure
# ============================================================

print("Remaining missing values:")
print(df.isnull().sum().sort_values(ascending=False).head())

print("\nSummary statistics:")
print(df.describe())

print("\nFinal dataset shape:", df.shape)

"""**Step #12** performs a final data‑quality validation to ensure the dataset is clean, complete, and structurally sound before moving into modeling or analysis. It begins by reporting any remaining missing values, allowing you to confirm that earlier imputation steps were successful. It then prints summary statistics for all numeric features, providing a quick overview of distributions, ranges, and potential anomalies after scaling. Finally, it displays the dataset’s final shape, confirming the number of rows and columns after all preprocessing, feature engineering, and transformations. This step acts as a final checkpoint, verifying that the dataset is ready for downstream modeling.

**Why This Step Matters**
A final validation step is essential for ensuring that the preprocessing pipeline produced a clean, consistent, and analysis‑ready dataset. Even small issues—such as lingering missing values, unexpected feature distributions, or incorrect dataset dimensions—can compromise model performance or lead to misleading results. By reviewing summary statistics and confirming the dataset’s final structure, you establish confidence that the data is stable, properly transformed, and ready for reliable modeling. This step also provides transparency and documentation, which are critical in clinical research workflows.
"""

# ============================================================
# 13. Export Final Cleaned Dataset
# ============================================================

df.to_csv(CLEANED_DATA_PATH, index=False)
print("Saved cleaned dataset to:", CLEANED_DATA_PATH)

"""**Step #13** exports the fully cleaned, transformed, and validated dataset to a designated output path, ensuring that the final version of the data is saved for downstream modeling, analysis, or sharing. Using df.to_csv() with index=False, the step writes the processed DataFrame to the predefined CLEANED_DATA_PATH, preserving the exact structure produced by the preprocessing pipeline. A confirmation message is printed to verify the save location, providing transparency and traceability within the workflow. This step marks the completion of the preprocessing phase and produces a stable, reusable dataset ready for modeling.

**Why This Step Matters**
Exporting the cleaned dataset is essential for reproducibility, collaboration, and workflow continuity. It creates a permanent, version‑controlled artifact that reflects all preprocessing decisions—imputation, encoding, scaling, feature engineering, and validation. Saving the dataset ensures that modeling steps can be run independently of preprocessing, prevents accidental reprocessing, and allows other team members or systems to use the exact same data. In clinical research pipelines, this step also supports auditability and documentation, both of which are critical for trustworthy results.
"""