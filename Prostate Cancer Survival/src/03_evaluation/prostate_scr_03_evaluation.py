# -*- coding: utf-8 -*-
"""Prostate_scr/03_evaluation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T1uOSnBVkwhlpRlt8tL97Rfa4b69a5H2
"""

# Step 1 — Mount Google Drive for File Access
from google.colab import drive
drive.mount('/content/drive')

"""**Step #1** initializes access to your Google Drive within the Colab environment by importing the google.colab library and mounting your Drive to the /content/drive directory. This action creates a secure connection between Colab and your stored files, allowing your notebook to read datasets, save outputs, and maintain a consistent file structure across sessions. If the drive is already mounted, Colab simply notifies you and offers the option to forcibly remount if needed.

**(Why This Step Matters)**
Mounting Google Drive is essential because it anchors your workflow to a stable, persistent storage location. Colab’s temporary runtime resets frequently, but your Drive does not—so mounting ensures that datasets, models, logs, and generated outputs are safely stored and retrievable across sessions. It also enables reproducibility, collaboration, and consistent directory paths throughout your pipeline.
"""

# Step 2 — Import Core Analysis Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.calibration import calibration_curve
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    average_precision_score, confusion_matrix, classification_report,
    brier_score_loss
)

"""**Step #2** loads all the essential Python libraries needed for data handling, numerical computation, visualization, and model evaluation. This includes pandas and numpy for data manipulation and numerical operations, matplotlib and seaborn for plotting, and a suite of functions from sklearn that support calibration curves, ROC analysis, precision‑recall evaluation, classification metrics, and Brier score calculations. By importing these libraries upfront, the notebook ensures that all analytical and visualization tools are available for the downstream workflow.

**(Why This Step Matters)**
Importing the correct libraries at the start ensures that your workflow is reproducible, organized, and free from mid‑notebook import errors. It also makes the analytical environment explicit, which is crucial for collaborators, reviewers, and future you. By centralizing all imports, you create a predictable foundation for data exploration, model evaluation, and figure generation—especially important in clinical research pipelines where transparency and consistency matter.
"""

# Step 3 — Load Cleaned and Imputed Dataset

import os

base_path = "/content/drive/MyDrive/"
data_path = os.path.join(base_path, "prostate_cancer_prediction_cleaned_imputed.csv")

df = pd.read_csv(data_path)
display(df.head())

"""**Step #3** loads the cleaned and imputed prostate cancer dataset from your Google Drive into a pandas DataFrame so it can be explored, analyzed, and used for modeling. The code constructs a file path using os.path.join, reads the CSV file with pd.read_csv, and displays the first few rows to confirm that the dataset has been successfully imported. This step establishes the core working dataset that the rest of the pipeline will rely on.

**(Why This Step Matters)**
Loading the dataset is foundational because every subsequent step—data exploration, feature engineering, model training, evaluation, and reporting—depends on having the correct, clean, and complete data in memory. By explicitly loading the cleaned and imputed version, you ensure that missing values have already been addressed and that the dataset is ready for analysis without interruptions. This step also confirms that the file path is correct and that the environment is properly connected to your Drive, preventing downstream errors.
"""

# Step 4 Verify Project Directory Structure

directory_path = "/content/drive/MyDrive/prostate_cancer_prediction/"
if os.path.exists(directory_path):
    contents = os.listdir(directory_path)
    print(f"Contents of '{directory_path}':")
    for item in contents:
        print(item)
else:
    print(f"Directory '{directory_path}' does not exist.")

"""**Step #4** checks whether the designated project directory exists in your Google Drive and, if it does, lists all items inside it. The code uses os.path.exists to confirm the directory’s presence and os.listdir to display its contents, which include subfolders such as data, models, and outputs. This step ensures that your project environment is correctly organized and that all expected folders are available before proceeding with further analysis or model development.

**(Why This Step Matters)**
This step is crucial because a well‑structured project directory underpins the entire workflow. Many later steps—loading data, saving figures, writing outputs, or accessing trained models—depend on these folders being present and correctly named. By validating the directory early, you prevent downstream errors, ensure reproducibility, and maintain a clean, organized research environment. In clinical research pipelines, this kind of structural verification is especially important for traceability and auditability.
"""

# Step 5 — Define True Labels and Predicted Probabilities
y_true = df["Biopsy_Result"].map({'Benign': 0, 'Malignant': 1})
y_pred_prob = df["Cancer_Stage"] # Assuming 'Cancer_Stage' is your predicted probability column. Please correct if wrong.

"""**Step #5** establishes the two essential components needed for evaluating a binary classification model: the true outcome labels and the model’s predicted probabilities. The code converts the biopsy results into numerical values—mapping Benign to 0 and Malignant to 1—to create y_true, the ground‑truth target variable. It then assigns y_pred_prob to the column containing the model’s predicted probabilities. Together, these variables form the foundation for computing ROC curves, calibration curves, precision‑recall metrics, and other performance evaluations.

**(Why This Step Matters)**
This step is critical because all downstream evaluation metrics—AUC, ROC, PR curves, calibration, confusion matrices, and more—depend on having correctly defined true labels and predicted probabilities. Mislabeling or mismatching these vectors can invalidate the entire evaluation process. By explicitly defining y_true and y_pred_prob early, you ensure reproducibility, clarity, and methodological integrity, which is especially important in clinical prediction research where interpretability and accuracy are paramount.
"""

# Step 6 Review Dataset Column Structure

print(df.columns)

"""**Step #6** prints all column names in the dataset to give you a complete overview of the available variables before performing any analysis or modeling. By calling df.columns, the notebook displays every feature in the DataFrame—including demographic attributes, clinical indicators, symptoms, lifestyle factors, and outcome variables—allowing you to verify that the dataset loaded correctly and contains the expected fields. This quick inspection ensures that you understand the structure and scope of the data you’re working with.

**(Why This Step Matters)**
This step is essential because understanding the dataset’s structure is the foundation of any reliable analysis. Before you can clean data, engineer features, or build predictive models, you must know exactly which variables exist and how they are named. Printing the column list helps catch typos, unexpected missing fields, inconsistent naming conventions, or additional variables that may influence modeling decisions. In clinical research workflows, this transparency is especially important for reproducibility, documentation, and ensuring that downstream steps use the correct inputs.
"""

# Step 7 — Convert Cancer Stage to Probability Scores
cancer_stage_to_prob_map = {
    'Localized': 0.2,       # Lower probability of malignancy
    'Advanced': 0.6,        # Medium probability
    'Metastatic': 0.9       # Higher probability
}

df['Cancer_Stage_Prob'] = df['Cancer_Stage'].map(cancer_stage_to_prob_map)

# Now, update y_pred_prob to use this new numerical column
y_pred_prob = df['Cancer_Stage_Prob']

# Display the first few rows with the new probability column to verify
display(df[['Cancer_Stage', 'Cancer_Stage_Prob']].head())

# Also check if there are any NaN values introduced if a stage was not in the map
if y_pred_prob.isnull().any():
    print("Warning: NaN values found in y_pred_prob. Check if all Cancer_Stage categories are covered in the map.")

"""**Step #7** converts the categorical Cancer_Stage variable into a numerical probability scale so that it can be used as a continuous prediction input for model evaluation. A mapping dictionary assigns assumed probability values to each stage—lower for Localized, moderate for Advanced, and higher for Metastatic. The mapped values are stored in a new column, Cancer_Stage_Prob, which then replaces the earlier prediction vector. The step also includes a validation check to ensure no unmapped categories introduce missing values.

**(Why This Step Matters)**
This step is essential because most evaluation metrics—ROC curves, calibration curves, precision‑recall curves, and Brier scores—require continuous probability predictions rather than categorical labels. By converting cancer stages into numeric values, you create a usable prediction vector that aligns with clinical severity while enabling quantitative model assessment. This also ensures consistency and interpretability across downstream analyses, especially in clinical research where transparent mapping decisions must be documented.
"""

# Step 8 — Evaluate Model Calibration with Calibration Curve
prob_true, prob_pred = calibration_curve(y_true, y_pred_prob, n_bins=10)

plt.figure(figsize=(7,7))
plt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')
plt.plot([0,1], [0,1], linestyle='--', color='gray', label='Perfect Calibration')
plt.xlabel("Predicted Probability")
plt.ylabel("Observed Probability")
plt.title("Calibration Curve — Prostate Cancer Model")
plt.legend()
plt.grid(True)
plt.show()

"""Step #8 computes and visualizes the calibration curve for the prostate cancer prediction model by comparing predicted probabilities (y_pred_prob) with the actual observed outcomes (y_true). Using calibration_curve, the code bins predictions into ten groups and calculates how well the predicted probabilities align with real‑world event frequencies. The resulting plot overlays the model’s calibration line against a perfect‑calibration diagonal, providing a visual assessment of whether the model tends to overestimate or underestimate risk.


"""

# Step 9 — Calculate Brier Score for Probability Accuracy
brier = brier_score_loss(y_true, y_pred_prob)
print("Brier Score:", round(brier, 4))

"""**Step #9** calculates the Brier Score, a key metric for evaluating the accuracy of probabilistic predictions in binary classification. Using brier_score_loss, the code compares each predicted probability (y_pred_prob) with the corresponding true outcome (y_true) and computes the mean squared difference between them. The resulting value—printed to four decimal places—quantifies how close the model’s predicted probabilities are to the actual outcomes, with lower scores indicating better calibration and overall probability accuracy.

**(Why This Step Matters)**
The Brier Score is one of the most interpretable and clinically meaningful metrics for evaluating risk‑prediction models. Unlike metrics that focus solely on ranking (such as AUC), the Brier Score evaluates both calibration and accuracy of the predicted probabilities themselves. In clinical decision‑making—especially in cancer risk assessment—probability estimates must be trustworthy, not just correctly ordered. A well‑calibrated, low‑Brier‑Score model provides more reliable risk communication and supports safer, evidence‑based decisions.
"""

# Step 10 — Evaluate Model Discrimination with ROC Curve and AUROC
fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)
auc = roc_auc_score(y_true, y_pred_prob)

plt.figure(figsize=(7,7))
plt.plot(fpr, tpr, label=f"AUROC = {auc:.3f}")
plt.plot([0,1], [0,1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Prostate Cancer Model")
plt.legend()
plt.grid(True)
plt.show()

"""**Step #10** evaluates the model’s ability to discriminate between malignant and benign cases by generating the Receiver Operating Characteristic (ROC) curve and calculating the Area Under the ROC Curve (AUROC). The code computes the false‑positive rate (FPR), true‑positive rate (TPR), and thresholds using roc_curve, then calculates the AUROC score with roc_auc_score. A plot is produced showing the model’s ROC curve alongside a diagonal reference line representing random performance, allowing you to visually assess how well the model separates positive from negative outcomes across all probability thresholds.

**(Why This Step Matters)**
The ROC curve and AUROC are foundational metrics for evaluating binary classifiers, especially in clinical prediction tasks. While calibration assesses probability accuracy, the ROC curve measures how well the model distinguishes between malignant and benign cases across all thresholds. AUROC summarizes this discrimination ability into a single number: values near 0.5 indicate random performance, while values closer to 1.0 reflect strong separation. In clinical settings, understanding discrimination is essential for determining whether a model is reliable enough to support screening, triage, or diagnostic decisions.
"""

# Step 11 Validate Data Types for Evaluation Inputs

print(f"Data type of y_true: {y_true.dtype}")
print(f"Data type of y_pred_prob: {y_pred_prob.dtype}")

"""**Step #11** prints the data types of the two core evaluation variables—y_true and y_pred_prob—to confirm they are in the correct numerical formats required for model‑evaluation metrics. The output shows that y_true is stored as int64 and y_pred_prob as float64, which is the appropriate pairing for binary classification tasks: integer‑encoded true labels and floating‑point predicted probabilities. This quick check ensures that the evaluation pipeline will run smoothly without type‑related errors.

**(Why This Step Matters)**
Data‑type validation is a small but essential quality‑control step in any modeling workflow. Many evaluation functions require specific input types—true labels must be integers (0/1), and predicted probabilities must be floats between 0 and 1. If these types are incorrect, metrics may fail silently, produce misleading results, or raise errors later in the pipeline. By verifying data types early, you safeguard the integrity of your evaluation process and ensure reproducibility, which is especially important in clinical prediction research.
"""

# Step 12 — Evaluate Model Performance with Precision–Recall Curve and AUPRC
precision, recall, thresholds = precision_recall_curve(y_true, y_pred_prob)
auprc = average_precision_score(y_true, y_pred_prob)

plt.figure(figsize=(7,7))
plt.plot(recall, precision, label=f"AUPRC = {auprc:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Prostate Cancer Model")
plt.legend()
plt.grid(True)
plt.show()

"""**Step #12** evaluates the model’s performance in distinguishing malignant from benign cases under varying probability thresholds by generating the Precision–Recall (PR) curve and computing the Area Under the Precision–Recall Curve (AUPRC). The code calculates precision, recall, and threshold values using precision_recall_curve, then computes the AUPRC with average_precision_score. The resulting plot visualizes how precision changes as recall increases, offering insight into the model’s behavior in imbalanced or clinically sensitive settings where false negatives and false positives carry different consequences.

**(Why This Step Matters)**
The Precision–Recall curve is especially important in clinical prediction tasks where the positive class—such as malignant cancer—is relatively rare or carries high clinical risk. Unlike ROC curves, PR curves focus directly on the model’s ability to correctly identify true positives while minimizing false alarms. AUPRC provides a more informative summary metric than AUROC in imbalanced datasets, making it crucial for evaluating whether a model is genuinely useful for early detection, triage, or diagnostic support. This step ensures that the model’s performance is assessed in a way that aligns with real‑world clinical priorities.
"""

# Step 13 — Stratify Patients by Predicted Risk
bins = [-np.inf, 0.4, np.inf] # Define custom bins to separate 0.2 from 0.6 and 0.9
labels = ["Low", "High"]
df["y_true"] = y_true # Add y_true as a column to df
df["risk_group"] = pd.cut(y_pred_prob, bins=bins, labels=labels, right=True)

risk_table = df.groupby("risk_group", observed=True)["y_true"].agg(["count", "mean"])
risk_table.rename(columns={"mean": "Observed Risk"}, inplace=True)

print("Risk Stratification Table:")
print(risk_table)

"""**Step #13** assigns each patient to a risk group based on their predicted probability of malignancy and then evaluates how well these groups reflect actual observed outcomes. The code defines probability bins (e.g., below 0.4 as Low risk and above 0.4 as High risk), adds the true labels to the DataFrame, and uses pd.cut to categorize each prediction into a risk group. It then computes the number of patients and the observed malignancy rate within each group, producing a concise risk‑stratification table that summarizes how predicted risk aligns with real outcomes.

**(Why This Step Matters)**
Risk stratification is a critical step in clinical prediction modeling because it translates raw probability outputs into actionable categories that clinicians can use for decision‑making. While metrics like AUROC or AUPRC evaluate model performance statistically, risk groups show how predictions behave in practice—whether “high‑risk” patients truly exhibit higher observed malignancy rates, and whether thresholds are clinically sensible. This step bridges the gap between model evaluation and real‑world application, supporting triage decisions, follow‑up planning, and patient communication.
"""

# Step 14 — Visualize Observed Risk by Risk Group
plt.figure(figsize=(8,6))
sns.barplot(x=risk_table.index, y=risk_table["Observed Risk"])
plt.title("Observed Cancer Risk by Risk Group")
plt.ylabel("Observed Risk")
plt.xlabel("Risk Group")
plt.ylim(0,1)
plt.grid(axis='y')
plt.show()

"""**Step #14** creates a bar plot that visually compares the observed cancer risk across the previously defined risk groups. Using seaborn’s barplot, the code displays the mean observed malignancy rate for each group—Low and High—based on the risk‑stratification table generated in Step #13. The plot includes labeled axes, a descriptive title, a fixed y‑axis range from 0 to 1, and horizontal grid lines to improve readability. This visualization provides an intuitive, at‑a‑glance understanding of how well the predicted risk categories align with actual outcomes.

**(Why This Step Matters)**
This step is important because it transforms numerical risk‑stratification results into a visual format that is easier to interpret and communicate—especially in clinical or stakeholder settings. While tables provide precise values, visualizations reveal patterns immediately, such as whether higher predicted risk truly corresponds to higher observed malignancy rates. This helps validate the usefulness of the risk‑group thresholds and supports transparent, evidence‑based decision‑making in clinical prediction workflows.
"""

# Step 15 — Evaluate Classification Outcomes with Confusion Matrix
threshold = 0.5
y_pred = (y_pred_prob >= threshold).astype(int)

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f"Confusion Matrix (Threshold = {threshold})")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""**Step #15** evaluates the model’s classification performance at a fixed probability threshold by converting predicted probabilities into binary predictions and generating a confusion matrix. Using a threshold of 0.5, the code classifies each case as malignant or benign, computes the confusion matrix with confusion_matrix(y_true, y_pred), and visualizes the results as a heatmap. The resulting matrix displays the counts of true negatives, false positives, false negatives, and true positives, providing a direct snapshot of how the model performs when forced to make yes/no decisions.

**(Why This Step Matters)**
The confusion matrix is essential because it reveals the real‑world consequences of the model’s predictions. Unlike ROC or PR curves, which summarize performance across all thresholds, the confusion matrix shows exactly how many patients are misclassified at a specific decision threshold. In clinical prediction tasks, false negatives (missed malignancies) and false positives (unnecessary alarms) carry very different risks. Understanding this balance is crucial for determining whether the model is safe, reliable, and appropriate for clinical use.
"""

# Step 16 — Evaluate Model Performance with Classification Report
print(classification_report(y_true, y_pred))

"""**Step #16** produces a full classification report summarizing the model’s performance across key metrics—precision, recall, F1‑score, and support—for each class. Using classification_report(y_true, y_pred), the code evaluates how well the model distinguishes between benign (0) and malignant (1) cases at the chosen threshold. The resulting table highlights performance differences between classes, overall accuracy, and macro‑ and weighted‑averaged metrics, offering a comprehensive snapshot of the model’s strengths and weaknesses.

**(Why This Step Matters)**
The classification report is essential because it reveals performance asymmetries that single metrics like accuracy or AUROC can hide. In clinical prediction tasks, the minority or high‑risk class (e.g., malignant cases) often matters most, and poor recall or precision for that class can have serious consequences. This step exposes whether the model is disproportionately favoring one class, whether it struggles to detect malignancies, and whether its predictions are balanced enough for real‑world use. It provides the diagnostic detail needed to judge model reliability and fairness.
"""

# Step 17 — Export Evaluation Results to CSV
output_path = "/content/drive/MyDrive/prostate_cancer_prediction/evaluation_results.csv"
risk_table.to_csv(output_path)
print("Saved:", output_path)

"""**Step #17** saves the evaluation results—specifically the risk_table generated during risk stratification—to a CSV file in your project directory on Google Drive. The code defines the output path, writes the DataFrame to that location using to_csv, and prints a confirmation message indicating where the file was saved. This ensures that the results of your model evaluation are stored persistently and can be accessed later for reporting, auditing, or further analysis.

**(Why This Step Matters)**
Saving evaluation outputs is essential for reproducibility, documentation, and downstream workflows. In clinical prediction research, results must be traceable, shareable, and stored in a stable location—especially when collaborating with others or preparing manuscripts, reports, or regulatory submissions. By exporting the risk‑stratification table, you ensure that key evaluation metrics are preserved independently of the notebook session, which is crucial given Colab’s temporary runtime environment.
"""