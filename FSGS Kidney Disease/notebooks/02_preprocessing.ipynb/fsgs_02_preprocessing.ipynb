{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxD7EZPvNplz",
        "outputId": "f25f412d-fe7b-4496-c186-9723ebb33083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#1 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Load dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/fsgs_dataset_cleaned.csv')"
      ],
      "metadata": {
        "id": "EhPLqqyIOAMn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Drop irrelevant columns\n",
        "df.drop(columns=['Patient_ID', 'Notes'], inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "abcZBl54OATM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "9DKO1HsLOQku"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit and transform the data, which returns a NumPy array\n",
        "imputed_data = imputer.fit_transform(df)\n",
        "\n",
        "# Get the names of the features that were actually processed and output by the imputer.\n",
        "# This accounts for columns skipped due to being all NaN.\n",
        "processed_columns = imputer.get_feature_names_out(df.columns)\n",
        "\n",
        "# Create a new DataFrame from the imputed NumPy array,\n",
        "# explicitly retaining the correct column names and index.\n",
        "df = pd.DataFrame(imputed_data, columns=processed_columns, index=df.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK_-uLXuOQ0t",
        "outputId": "22822b16-045f-4199-c40b-e93fc7db9ec5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['R_vs_NR' 'CE' 'CF' 'CG' 'CH' 'CI' 'CJ' 'CK' 'CL' 'CM' 'CN' 'CO']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Normalize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[df.columns] = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "fOWVQxzYOiqF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Save preprocessed data\n",
        "df.to_csv('/content/drive/MyDrive/projects/fsgs/notebooks/fsgs_dataset_preprocessed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "25NASiO9ONaX"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}