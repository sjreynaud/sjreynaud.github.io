{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxD7EZPvNplz",
        "outputId": "d0501058-df81-49c4-b81d-9a049069c38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#1 Mount Google Drive / Connect to Google Drive for File Access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#1:** In this initial step, the code mounts the user's Google Drive to the Colab environment, enabling seamless access to files stored in Drive. By importing the drive module from google.colab and executing drive.mount('/content/drive'), the notebook establishes a virtual link between Colab and the user's Drive. This is essential for loading datasets, saving outputs, or accessing project resources directly from cloud storage. Once mounted, the Drive appears under /content/drive, and the confirmation message \"Mounted at /content/drive\" signals successful integration."
      ],
      "metadata": {
        "id": "FMURICOZItBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Load dataset / Import Cleaned FSGS Dataset into DataFrame\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/fsgs_dataset_cleaned.csv')"
      ],
      "metadata": {
        "id": "EhPLqqyIOAMn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#2:** This step loads a cleaned dataset into the Colab environment using the pandas library, a foundational tool for data manipulation in Python. By importing pandas as pd and calling pd.read_csv() with the path to the CSV file stored in Google Drive, the code reads the contents of fsgs_dataset_cleaned.csv into a DataFrame named df. This structure allows for efficient data exploration, filtering, and analysis. The file path reflects the integration from Step #1, ensuring seamless access to Drive-hosted resources."
      ],
      "metadata": {
        "id": "wc8gw-nWJJsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Drop irrelevant columns / Remove Non-Analytic Columns for Clean Modeling\n",
        "df.drop(columns=['Patient_ID', 'Notes'], inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "abcZBl54OATM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#3:** This step performs a data cleaning operation by removing columns that are deemed irrelevant for analysis. Specifically, the code uses df.drop(columns=['Patient_ID', 'Notes'], inplace=True, errors='ignore') to eliminate the 'Patient_ID' and 'Notes' columns from the DataFrame df. The inplace=True argument ensures the changes are applied directly to df without creating a copy, while errors='ignore' prevents the code from breaking if either column is missing. This streamlines the dataset by discarding identifiers and unstructured text that may not contribute to modeling or statistical analysis."
      ],
      "metadata": {
        "id": "l_jL4cmlJ2Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Encode categorical variables / One-Hot Encode Categorical Features for Modeling\n",
        "df = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "9DKO1HsLOQku"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#4:** This step transforms categorical variables in the dataset into a format suitable for machine learning models. Using pd.get_dummies(df, drop_first=True), the code replaces categorical columns with binary indicator variables, a process known as one-hot encoding. The drop_first=True parameter avoids multicollinearity by omitting the first category in each encoded feature, ensuring the resulting DataFrame is numerically optimized for regression or classification tasks. This encoding step is crucial for converting qualitative data into quantitative form without introducing redundancy."
      ],
      "metadata": {
        "id": "jrTgQHB1KWUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5  Configure Median-Based Imputation for Missing Values\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')"
      ],
      "metadata": {
        "id": "ME7GDY3ZKw85"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#5:** This step sets up the imputation strategy for handling missing values in the dataset. By importing SimpleImputer from sklearn.impute and instantiating it with strategy='median', the code prepares a tool that will later replace missing entries in numerical columns with their respective median values. This approach is robust against outliers and preserves the central tendency of each feature. The reminder to import pandas ensures compatibility with DataFrame operations, which are essential for applying the imputer in subsequent steps."
      ],
      "metadata": {
        "id": "IyVBNuR6Lrgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Fit and transform the data, which returns a NumPy array / Apply Median Imputation to Fill Missing Values\n",
        "imputed_data = imputer.fit_transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V17nrKu6K4t5",
        "outputId": "13377795-99d6-4429-9f49-79dcc70a6e84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['R_vs_NR' 'CE' 'CF' 'CG' 'CH' 'CI' 'CJ' 'CK' 'CL' 'CM' 'CN' 'CO']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#6:** This step applies the median-based imputation strategy configured earlier to the dataset. By executing imputed_data = imputer.fit_transform(df), the code fits the SimpleImputer to the DataFrame df, calculating the median for each column with missing values, and then replaces those missing entries accordingly. The result is a NumPy array (imputed_data) containing the fully imputed data. This transformation ensures that the dataset is numerically complete and ready for downstream modeling, while preserving the central tendency of each feature."
      ],
      "metadata": {
        "id": "As23kpDJMRyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Get the names of the features that were actually processed and output by the imputer./ Extract Imputed Feature Names\n",
        "# This accounts for columns skipped due to being all NaN.\n",
        "processed_columns = imputer.get_feature_names_out(df.columns)"
      ],
      "metadata": {
        "id": "N6774cnoLGKh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#7:** This step extracts the names of the columns that were successfully processed by the imputer using imputer.get_feature_names_out(df.columns). It ensures that only the columns with valid (non-all-NaN) data are included in the output, which is crucial for maintaining consistency between the input and the transformed dataset. Columns that were entirely missing (all NaN) are automatically excluded by the imputer, and this method reflects that exclusion by returning only the names of the retained features."
      ],
      "metadata": {
        "id": "Eok8-mjygB1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Create a new DataFrame from the imputed NumPy array / Rebuild DataFrame with Imputed Values\n",
        "# explicitly retaining the correct column names and index.\n",
        "df = pd.DataFrame(imputed_data, columns=processed_columns, index=df.index)"
      ],
      "metadata": {
        "id": "w5nejR77K5Go"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#8:** This step reconstructs a pandas DataFrame from the imputed NumPy array (imputed_data) by explicitly assigning the correct column names (processed_columns) and preserving the original row index (df.index). This ensures that the transformed data remains aligned with the original dataset structure, which is critical for downstream analysis, merging, or interpretation. Without this step, the imputed data would lack meaningful labels and indexing, making it harder to trace or validate."
      ],
      "metadata": {
        "id": "FyCsTXfqglhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Normalize features / Standardize Features with Z-Score Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[df.columns] = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "fOWVQxzYOiqF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#9:** This step standardizes all numerical features in the DataFrame using scikit-learn’s StandardScaler, which transforms each column to have a mean of 0 and a standard deviation of 1. This normalization is essential for many machine learning algorithms that are sensitive to the scale of input features, such as logistic regression, support vector machines, and k-nearest neighbors. By applying scaler.fit_transform(df) and reassigning the result to df[df.columns], the code ensures that the scaled values replace the original ones while preserving the DataFrame’s structure."
      ],
      "metadata": {
        "id": "YtO9cWCzhR6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Save preprocessed data / Export Final Dataset to CSV\n",
        "df.to_csv('/content/drive/MyDrive/projects/fsgs/notebooks/fsgs_dataset_preprocessed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "25NASiO9ONaX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#10:** This step finalizes the preprocessing pipeline by exporting the cleaned and transformed DataFrame to a CSV file using df.to_csv(...). The file is saved to a designated path within the user's Google Drive, and index=False ensures that the row indices are excluded from the output file. This makes the dataset ready for downstream tasks such as modeling, sharing, or archival. Saving at this stage preserves all prior transformations—imputation, scaling, and structural alignment—into a reproducible format."
      ],
      "metadata": {
        "id": "t1Kq_9Olh2Xj"
      }
    }
  ]
}