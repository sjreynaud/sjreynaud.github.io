# -*- coding: utf-8 -*-
"""FSGS_ 01_test_preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_jXatT7JiclsA0cIZQWacDDngzjeGuH

ðŸ“ 01_test_preprocessing.py
"""

#1 Import Pandas for Data Handling

import pandas as pd

"""**Step#1:** This step imports the Pandas library, a foundational tool in Python for data manipulation and analysis. The alias pd is a widely accepted convention that simplifies function calls like pd.DataFrame() or pd.read_csv(). By importing Pandas early, the script ensures access to powerful data structures and functions for handling tabular data throughout the pipeline."""

#2 Validate Cleaned Dataset Integrity

def test_load_dataset():
    path = '/content/drive/MyDrive/fsgs_Kidney_Disease_study/data/processed/fsgs_dataset_cleaned.csv'
    df = pd.read_csv(path)
    assert not df.empty, "Dataset is empty"
    assert df.shape[1] > 1, "Dataset has insufficient columns"

"""**Step#2:** This step defines a test function to validate the integrity of the cleaned dataset before further analysis. It loads the CSV file from a specified path using Pandas and performs two critical assertions: one to confirm the dataset is not empty, and another to ensure it contains more than one column. These checks act as a safeguard against incomplete or corrupted data, ensuring that downstream processes operate on a valid dataset."""

#3 Check Missing Value Thresholds

def test_missing_values():
    df = pd.read_csv('/content/drive/MyDrive/fsgs_Kidney_Disease_study/data/processed/fsgs_dataset_cleaned.csv')
    missing_ratio = df.isnull().mean()
    assert all(missing_ratio < 0.05), "Too many missing values in some columns"

"""**Step#3:** This function checks the proportion of missing values in each column of the cleaned dataset. It reads the CSV file and computes the missing value ratio using df.isnull().mean(). The assertion ensures that all columns have less than 5% missing data, which is a common threshold for acceptable data quality. If any column exceeds this limit, the function raises an error, flagging the dataset as needing further cleaning or imputation before analysis."""

#4 Verify Presence of Categorical Features

def test_column_types():
    df = pd.read_csv('/content/drive/MyDrive/fsgs_Kidney_Disease_study/data/processed/fsgs_dataset_cleaned.csv')
    assert df.select_dtypes(include=['object']).shape[1] > 0, "No categorical columns detected"

"""**Step#4:** This function verifies that the dataset contains at least one categorical column, which is essential for many modeling and preprocessing tasks such as encoding or stratification. It reads the cleaned CSV file and uses select_dtypes(include=['object']) to isolate columns with object-type data, typically representing categorical variables. If none are found, the assertion fails, signaling that the dataset may lack key features needed for classification or subgroup analysis."""