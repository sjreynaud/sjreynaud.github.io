# -*- coding: utf-8 -*-
"""FSGS_src/04_visualization: Kaplan-Meier Curves & Hazard Ratios

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_JAI-1auQVeXZbjmuNNhxuuuGEFAbKf7

# 04_visualization: Kaplan-Meier Curves & Hazard Ratios
"""

# 1. Initialize Analysis Environment
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler

!pip install lifelines
from lifelines import KaplanMeierFitter, CoxPHFitter

"""**Step#1:** sets up the computational environment required for the survival‑analysis workflow. It loads essential Python libraries for data handling (pandas), visualization (matplotlib, seaborn), and preprocessing (LabelEncoder, StandardScaler). It also installs and imports the lifelines package, which provides the Kaplan–Meier and Cox proportional hazards models used later in the analysis. This step ensures that all tools needed for data manipulation, plotting, and survival modeling are available before any dataset is loaded or processed."""

# 2. Mount Google Drive for File Access
# ------------------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

"""**Step#2:** connects your Google Colab notebook to your Google Drive so the notebook can access datasets, scripts, and output folders stored there. By importing google.colab.drive and running drive.mount('/content/drive'), Colab creates a secure link between the session and your Drive account. Once mounted, all Drive files become available under the /content/drive directory, allowing you to load data directly into your analysis pipeline and save results back to persistent storage."""

# 3. Define Project File Paths
# ------------------------------------------------------------
project_root = '/content/drive/MyDrive/fsgs_Kidney_Disease_study'
data_path = os.path.join(project_root, "data", "processed", "fsgs_dataset_cleaned.csv")
output_path = os.path.join(project_root, "outputs", "visualizations")

os.makedirs(output_path, exist_ok=True)

print("Data Path:", data_path)
print("Output Path:", output_path)

"""**Step#3:** establishes the directory structure your workflow will rely on by defining the project’s root folder, the exact location of the cleaned dataset, and the output directory where visualizations will be saved. Using os.path.join ensures paths are constructed reliably across environments, and os.makedirs(output_path, exist_ok=True) guarantees that the output folder exists before any figures or analysis results are written. This step creates a stable, organized foundation so the rest of the pipeline can load data and store results without manual file handling."""

# 4. Load Cleaned Dataset
# ------------------------------------------------------------
df = pd.read_csv(data_path)

"""**Step#4:** loads the cleaned dataset into memory by reading the CSV file located at the path defined earlier in the workflow. Using pd.read_csv(data_path), the dataset is imported as a pandas DataFrame, giving you a structured, tabular object that can be explored, transformed, and analyzed in later steps. This marks the moment when the pipeline transitions from environment setup to working directly with real data."""

#5 Standardize Column Names
df.columns = df.columns.str.strip().str.lower()

print("Columns:", df.columns.tolist())

"""**Step#5:** cleans and standardizes the dataset’s column names to ensure consistency throughout the analysis pipeline. By stripping whitespace and converting all column names to lowercase, this step eliminates formatting inconsistencies that can cause errors later when selecting variables, merging datasets, or referencing columns in modeling code. The printed output confirms that the cleaned names are now uniform, predictable, and ready for downstream preprocessing and survival‑analysis tasks."""

# 6. Identify Survival Analysis Variables
# ------------------------------------------------------------
event_candidates = ["event", "death_y_n", "death", "status", "event_status"]
duration_candidates = ["duration", "fu_month", "time", "survival_months", "followup_time"]

event_col = next((c for c in event_candidates if c in df.columns), None)
duration_col = next((c for c in duration_candidates if c in df.columns), None)

if event_col is None or duration_col is None:
    raise ValueError("Could not find event/duration columns in dataset.")

df = df.rename(columns={event_col: "event", duration_col: "duration"})

# Ensure event is binary
df["event"] = df["event"].astype(int)

"""**Step#6:** automatically identifies which columns in the dataset represent the survival event (e.g., death, status) and the duration (e.g., follow‑up time, months). It searches through a list of common candidate names and selects the first match found in the DataFrame. If either column is missing, the code raises an error to prevent incorrect survival modeling. Once the correct columns are found, they are renamed to standardized labels—event and duration—and the event column is converted to an integer to ensure it behaves as a proper binary indicator for survival analysis."""

# 7. Prepare Covariates for Cox Model
# ------------------------------------------------------------
covariates = [
    "age",
    "proteinuria",
    "egfr",
    "creatinine",
    "blood_pressure",
    "sex",
    "race",
    "treatment_group"
]

existing_covariates = [c for c in covariates if c in df.columns]

cox_df = df[["duration", "event"] + existing_covariates].copy()

# Encode categorical variables
categorical_cols = ["sex", "race", "treatment_group"]
for col in categorical_cols:
    if col in cox_df.columns:
        le = LabelEncoder()
        cox_df[col] = le.fit_transform(cox_df[col])

# Scale numeric variables
numeric_cols = ["age", "proteinuria", "egfr", "creatinine", "blood_pressure"]
for col in numeric_cols:
    if col in cox_df.columns:
        scaler = StandardScaler()
        cox_df[col] = scaler.fit_transform(cox_df[[col]])

"""**Step#7:** prepares the dataset for fitting a Cox proportional hazards model by selecting relevant covariates, encoding categorical variables, and scaling numeric features. First, the code filters the dataset to include only the survival columns (duration, event) plus any covariates that actually exist in the DataFrame. Next, categorical variables such as sex, race, and treatment group are label‑encoded so the Cox model can interpret them numerically. Finally, continuous variables like age, proteinuria, eGFR, creatinine, and blood pressure are standardized using z‑score scaling, ensuring that all numeric predictors are on comparable scales and preventing any single variable from dominating the model due to magnitude differences."""

# 8. Fit Cox Proportional Hazards Model
# ------------------------------------------------------------
cph = CoxPHFitter()
cph.fit(cox_df, duration_col="duration", event_col="event")

print("\nCox Model Summary:")
cph.print_summary()

"""**Step#8:** fits the Cox Proportional Hazards model using the prepared survival dataset. The code initializes a CoxPHFitter, trains it on the DataFrame containing duration, event, and all selected covariates, and then prints a detailed model summary. This summary includes hazard ratios, confidence intervals, p‑values, concordance, and log‑likelihood statistics—key outputs that help you interpret which variables significantly influence the risk of experiencing the event. This step marks the transition from data preparation to actual statistical modeling."""

# 9. Stratify Patients into Risk Tertiles
# ------------------------------------------------------------
cox_df["prognostic_score"] = cph.predict_partial_hazard(cox_df[existing_covariates])

tertiles = cox_df["prognostic_score"].quantile([1/3, 2/3])

def assign_group(score):
    if score <= tertiles.iloc[0]:
        return "Low"
    elif score <= tertiles.iloc[1]:
        return "Medium"
    else:
        return "High"

cox_df["risk_group"] = cox_df["prognostic_score"].apply(assign_group)

print("\nRisk Group Distribution:")
print(cox_df["risk_group"].value_counts())

"""**Step#9:** uses the fitted Cox model to generate a prognostic score for each patient and then divides those scores into tertiles to form three clinically interpretable risk groups: Low, Medium, and High. The prognostic score reflects each individual’s relative hazard based on their covariates. By computing tertiles, the step creates balanced groups that help visualize and compare survival patterns across different levels of predicted risk. The final output shows how many patients fall into each category, confirming that the groups are well‑distributed."""

# 10. Plot Kaplan–Meier Curves by Risk Group
# ------------------------------------------------------------
kmf = KaplanMeierFitter()
plt.figure(figsize=(8,6))

for group in ["Low", "Medium", "High"]:
    mask = cox_df["risk_group"] == group
    if not cox_df[mask].empty:
        kmf.fit(cox_df[mask]["duration"], event_observed=cox_df[mask]["event"], label=group)
        kmf.plot_survival_function()

plt.title("Kaplan-Meier Survival by Risk Group")
plt.xlabel("Time (months)")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "km_risk_groups.png"))
plt.show()

"""**Step#10:** visualizes survival differences across the previously created Low, Medium, and High risk groups by generating Kaplan–Meier survival curves. For each group, the code filters the dataset, fits a Kaplan–Meier estimator using the group’s duration and event data, and plots the resulting survival function. The combined plot clearly shows how survival probability changes over time for each risk category, allowing you to assess whether the Cox‑derived risk groups meaningfully separate patients based on predicted outcomes. The figure is then saved to the output directory and displayed for interpretation."""

# 11. Plot Cox Model Hazard Ratios
# ------------------------------------------------------------
plt.figure(figsize=(8,6))
cph.plot()
plt.title("Hazard Ratios from Cox Model")
plt.tight_layout()
plt.savefig(os.path.join(output_path, "hazard_ratios.png"))
plt.show()

"""**Step#11:** generates a hazard‑ratio visualization from the fitted Cox model, allowing you to see how each covariate influences the risk of the event on a log‑scale. The code calls the model’s built‑in plotting function, which displays the estimated hazard ratios along with their 95% confidence intervals. This plot provides an intuitive way to assess which predictors increase or decrease risk and how precise those estimates are. The figure is then titled, formatted, saved to the output directory, and displayed for interpretation."""