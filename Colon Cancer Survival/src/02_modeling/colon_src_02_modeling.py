# -*- coding: utf-8 -*-
"""Colon_src/02_modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X9eDCWnZQKcXV2mSM48w71Wc_gxPcNj2
"""

# 1. Initialize Analysis Environment and Import Libraries

import sys
!{sys.executable} -m pip install lifelines


import pandas as pd
import numpy as np

# Survival analysis
from lifelines import CoxPHFitter, KaplanMeierFitter

# ML pipelines
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

"""**Step #1** initializes the entire analysis environment by importing all essential Python libraries needed for survival analysis, machine learning, data preprocessing, and visualization. It begins by ensuring that the lifelines package is installed, then loads core scientific libraries like pandas and numpy. Next, it brings in survival‚Äëanalysis tools (CoxPHFitter, KaplanMeierFitter), machine‚Äëlearning utilities from scikit‚Äëlearn (such as Pipeline, StandardScaler, LogisticRegression, and RandomForestClassifier), and visualization libraries (matplotlib and seaborn). This step sets the foundation for every subsequent operation in the workflow.

**Why This Step Matters**
Importing libraries at the beginning creates a stable, reproducible environment. It prevents runtime errors caused by missing packages, ensures consistent versions across collaborators, and makes the workflow easier to read and maintain. For clinical research pipelines‚Äîespecially those involving survival analysis‚Äîhaving the correct libraries loaded is essential for accurate modeling, plotting survival curves, and building reproducible ML pipelines.
"""

# 2. Connect Google Drive for Persistent File Access
from google.colab import drive
drive.mount('/content/drive')

"""**Step #2** connects your Google Colab environment to your Google Drive so that the notebook can access datasets, save outputs, and maintain a consistent file structure across sessions. By importing the google.colab drive module and mounting Drive at the /content/drive directory, Colab gains permission to read and write files stored in your Drive. Once mounted, all subsequent steps in your workflow can reliably load data, save models, and store figures without manual uploads.

**(Why This Step Matters)**
Colab sessions reset frequently, and local storage is temporary. Mounting Google Drive ensures that your data, models, logs, and generated figures remain accessible across sessions. For clinical research pipelines‚Äîespecially those involving survival analysis and machine‚Äëlearning workflows‚Äîpersistent storage is essential for reproducibility, collaboration, and long‚Äëterm project organization.
"""

import os

# 3. Set Up Project Directory Structure and File Paths

project_root = '/content/drive/MyDrive/colorectal_cancer_prediction'
data_path = os.path.join(project_root, "data", "processed", "colorectal_cancer_prediction_cleaned_imputed.csv")
output_path = os.path.join(project_root, "data", "preprocessed")
CLEANED_DATA_PATH = os.path.join(output_path, "colorectal_cancer_prediction_preprocessed.csv")

os.makedirs(output_path, exist_ok=True)

print("Project paths set.")
print("Data path:", data_path)
print("Output path:", output_path)
print("Cleaned data save path:", CLEANED_DATA_PATH)

"""**Step #3** establishes the project‚Äôs directory structure by defining the root folder, the location of the processed input dataset, and the output directory where preprocessed files will be saved. Using os.path.join, the code constructs platform‚Äësafe paths for both reading and writing data, then ensures that the output directory exists by creating it if necessary. This step concludes by printing the resolved paths, confirming that the workflow now has a stable, organized file system foundation for all subsequent data operations.

**(Why This Step Matters)**
A well‚Äëstructured file system is essential for reproducible research. Without clearly defined paths, notebooks become fragile‚Äîbreaking when moved, shared, or run on a different machine. For clinical research pipelines, where datasets, intermediate files, and final outputs must be traceable and well‚Äëorganized, this step ensures consistency, prevents accidental overwrites, and supports long‚Äëterm project maintainability.
"""

# 4. Define Base Directory and Dataset Path
import os

BASE_DIR = "/content/drive/MyDrive/colorectal_cancer_prediction"
DATA_PATH = os.path.join(BASE_DIR, "colorectal_cancer_prediction_cleaned_imputed.csv")

"""**Step #4** establishes the core project paths by defining the base directory where all project files are stored and constructing the full path to the cleaned, imputed dataset. By importing the os module and using os.path.join, this step ensures that file paths are created in a reliable, platform‚Äësafe way. These variables become the foundational references for loading data and managing files throughout the rest of the workflow, keeping the project organized and preventing hard‚Äëcoded, error‚Äëprone paths.

**(Why This Step Matters)**
Clear, consistent path definitions are essential for reproducible research. Without them, file‚Äëloading errors become common, especially when switching machines, collaborating, or reorganizing directories. For a clinical research pipeline‚Äîwhere traceability and structure matter‚Äîthis step ensures that every subsequent operation knows exactly where to find the input data, supporting stability and long‚Äëterm maintainability.
"""

# 5. Load and Verify Dataset Structure
# -----------------------------
df = pd.read_csv(data_path)

print("Dataset loaded successfully.")
print("Shape:", df.shape)
print("Columns:", list(df.columns))

"""**Step #5** loads the cleaned and imputed colorectal cancer dataset into memory using pandas.read_csv, creating a DataFrame that becomes the central object for all subsequent analysis. After loading the file, the code prints confirmation messages along with the dataset‚Äôs shape and column names, allowing you to immediately verify that the file was read correctly and that the expected variables are present. This quick structural check ensures that the dataset is intact, properly formatted, and ready for preprocessing, feature engineering, and modeling.

**(Why This Step Matters)**
Loading the dataset correctly is foundational for every downstream task. If the data is missing, malformed, or misaligned with expectations, later steps‚Äîsuch as preprocessing, modeling, and survival analysis‚Äîwill fail or produce misleading results. By validating the dataset structure upfront, you ensure reproducibility, reduce debugging time, and maintain the integrity of the clinical research pipeline.
"""

# 6. Load and Preview Cleaned Dataset
df = pd.read_csv(data_path)
df.head()

"""**Step #6** loads the cleaned dataset into a pandas DataFrame and immediately displays the first few rows to give a quick visual confirmation of the dataset‚Äôs structure and contents. By calling df.head(), you can inspect key variables such as demographic features, clinical history, diagnostic information, and recurrence outcomes. This early preview ensures that the dataset has been read correctly, that the columns appear as expected, and that the data is in a usable format before moving on to preprocessing or modeling.

**(Why This Step Matters)**
Inspecting the dataset right after loading is essential for maintaining data integrity. If the dataset is corrupted, misaligned, or missing key variables, downstream steps like preprocessing, feature engineering, and survival modeling will fail or produce misleading results. This quick validation step ensures that the workflow begins with a trustworthy dataset, supporting reproducibility and accuracy in clinical research pipelines.
"""

# 7. Define and Validate Survival Outcome Variables
DURATION_COL = "Time_to_Recurrence"     # e.g., days until recurrence or death
EVENT_COL = "Recurrence"                # 1 = event occurred, 0 = censored

# Convert 'Recurrence' to numerical (1 for 'Yes', 0 for 'No')
df[EVENT_COL] = df[EVENT_COL].map({'Yes': 1, 'No': 0})

# Ensure columns exist
assert DURATION_COL in df.columns, f"{DURATION_COL} missing"
assert EVENT_COL in df.columns, f"{EVENT_COL} missing"

"""**Step #7** defines the two essential variables required for survival analysis: the duration column (Time_to_Recurrence) and the event indicator column (Recurrence). The code converts the event column from categorical labels (‚ÄúYes‚Äù/‚ÄúNo‚Äù) into numerical values (1 for event occurred, 0 for censored), ensuring compatibility with survival‚Äëanalysis models. It then validates that both columns exist in the DataFrame using assertions, providing an early safeguard against missing or misnamed variables. This step prepares the dataset for downstream survival modeling by standardizing the core outcome structure.

**(Why This Step Matters)**
Survival analysis depends entirely on correctly defined duration and event variables. If these columns are missing, mislabeled, or improperly encoded, survival models will fail or produce invalid results. Converting the event column to numeric form ensures consistency and prevents subtle errors later in the pipeline. This step protects the integrity of the analysis and ensures that the dataset is structurally ready for Kaplan‚ÄìMeier estimation, Cox modeling, and other survival‚Äëbased methods.
"""

# 8. Fit Cox Proportional Hazards Model with Encoded Covariates
cox_df = df.copy()

# Identify categorical columns (object type) for one-hot encoding
categorical_cols = cox_df.select_dtypes(include=['object']).columns

# Exclude 'Patient_ID' if it's not a feature
if 'Patient_ID' in categorical_cols:
    categorical_cols = categorical_cols.drop('Patient_ID')

# Apply one-hot encoding to categorical columns
cox_df = pd.get_dummies(cox_df, columns=categorical_cols, drop_first=True)

cox = CoxPHFitter()
cox.fit(cox_df, duration_col=DURATION_COL, event_col=EVENT_COL)

cox.print_summary()

"""**Step #8** fits a Cox Proportional Hazards model to the dataset using the lifelines library, enabling survival analysis based on multiple covariates. It begins by identifying categorical variables and applying one‚Äëhot encoding to convert them into numeric format, excluding Patient_ID if present. The transformed dataset is then passed to CoxPHFitter, specifying the duration and event columns previously defined. The model is trained on over 89,000 observations, and a summary is printed to display coefficients, hazard ratios, confidence intervals, and p‚Äëvalues‚Äîoffering insight into which features significantly influence time to recurrence.

**(Why This Step Matters)**
The Cox model is a cornerstone of survival analysis, especially in clinical research. It enables multivariate assessment of risk factors while accounting for censored data. Without proper encoding and column selection, the model would fail or yield misleading results. This step transforms raw data into a format suitable for robust statistical modeling and generates interpretable outputs that inform clinical decision‚Äëmaking, risk stratification, and hypothesis testing.
"""

# 9. Plot Kaplan‚ÄìMeier Survival Curve
km = KaplanMeierFitter()

plt.figure(figsize=(8, 6))
km.fit(df[DURATION_COL], event_observed=df[EVENT_COL])
km.plot_survival_function()
plt.title("Kaplan‚ÄìMeier Survival Curve")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.show()

"""**Step #9** generates a Kaplan‚ÄìMeier survival curve using the KaplanMeierFitter from the lifelines library, providing a non‚Äëparametric estimate of the survival function over time. The code fits the model using the previously defined duration and event columns, then plots the resulting survival curve with clear labels, a title, and gridlines for readability. The resulting stepwise curve visually represents how survival probability declines as time progresses, offering an intuitive overview of recurrence‚Äëfree survival patterns in the dataset.

**(Why This Step Matters)**
The Kaplan‚ÄìMeier curve is one of the most fundamental tools in survival analysis. It provides a transparent, assumption‚Äëfree view of survival behavior before introducing multivariate models like the Cox PH model. This step allows researchers to validate the general shape of the survival distribution, detect anomalies, and communicate findings visually to clinicians, collaborators, or stakeholders. It also serves as a diagnostic check‚Äîif the curve looks unexpected, it may indicate issues with event coding, duration values, or data quality.

**SECTION B ‚Äî Machine Learning Pipelines**
"""

# 10. Define Machine‚ÄëLearning Target and Prepare Feature Set
TARGET = "Recurrence"   # example ‚Äî change to your actual target

assert TARGET in df.columns, f"{TARGET} missing"

X = df.drop(columns=[TARGET, DURATION_COL, EVENT_COL], errors="ignore")
y = df[TARGET]

"""**Step #10** defines the machine‚Äëlearning target variable and prepares the dataset for supervised modeling by separating the predictors from the outcome. The code sets "Recurrence" as the target column, verifies that it exists in the DataFrame, and then constructs the feature matrix x by dropping the target along with the survival‚Äëspecific columns (DURATION_COL and EVENT_COL). The target vector y is extracted directly from the designated target column. This step cleanly partitions the dataset into inputs and outputs, ensuring that the ML pipeline receives properly structured data.

**(Why This Step Matters)**
Correctly defining the target variable is essential for any supervised learning task. If the target is misidentified, missing, or accidentally included in the feature set, the model will produce invalid or misleading results. Separating features from the target also enforces good modeling hygiene, preventing leakage from outcome variables or survival‚Äëspecific fields. This step ensures that the ML pipeline begins with a clean, well‚Äëstructured dataset aligned with best practices in clinical prediction modeling.
"""

# 11. Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""**Step #11** takes the full feature matrix ùëã and target vector ùë¶ and splits them into training and testing subsets using train_test_split. In this case, 80% of the data is allocated for training the machine‚Äëlearning model, while 20% is reserved for evaluating its performance. The random_state=42 parameter ensures that the split is reproducible every time the code runs, and stratify=y guarantees that the class distribution in the target variable is preserved across both training and testing sets, which is especially important for classification tasks with imbalanced classes.

**(Why This Step Matters)**
A clean, leakage‚Äëfree feature set is essential for building trustworthy machine‚Äëlearning models. If the target or survival columns remain in the feature matrix, the model may appear to perform extremely well while actually learning from information it should not have access to. This step enforces good modeling hygiene, protects the validity of downstream performance metrics, and ensures that the ML pipeline reflects real‚Äëworld predictive conditions‚Äîcritical for clinical research and risk‚Äëprediction applications.
"""

# 12. Build and Train Logistic Regression Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Identify categorical and numerical columns from X_train
categorical_features = X_train.select_dtypes(include=['object']).columns
numerical_features = X_train.select_dtypes(include=np.number).columns

# Create a preprocessor to handle numerical scaling and categorical encoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough' # Keep any other columns not explicitly transformed
)

logreg_pipeline = Pipeline([
    ("preprocessor", preprocessor), # Add the preprocessor to the pipeline
    ("clf", LogisticRegression(max_iter=200))
])

logreg_pipeline.fit(X_train, y_train)

print("Logistic Regression Accuracy:", logreg_pipeline.score(X_test, y_test))

"""**Step #12** builds a complete machine‚Äëlearning pipeline for logistic regression by combining preprocessing and model training into a single, unified workflow. It first identifies which columns in the training data are categorical and which are numerical. Then it constructs a ColumnTransformer that applies StandardScaler to numerical features and OneHotEncoder to categorical features, ensuring all inputs are properly prepared for the model. This preprocessor is inserted into a Pipeline along with a LogisticRegression classifier, allowing the entire sequence‚Äîfrom transformation to prediction‚Äîto run automatically. Finally, the pipeline is trained on the training data and evaluated on the test set, producing an accuracy score.

**(Why This Step Matters)**
This step matters because it makes your machine‚Äëlearning workflow cleaner, safer, and more reproducible. Pipelines eliminate the risk of mismatched preprocessing between training and testing, reduce code duplication, and make your model easier to deploy. They also allow you to integrate hyperparameter tuning tools like GridSearchCV seamlessly, since the entire process‚Äîfrom encoding to scaling to modeling‚Äîis encapsulated in one object.
"""

# 13. Build and Train Random Forest Pipeline
rf_pipeline = Pipeline([
    ("preprocessor", preprocessor), # Add the preprocessor to the pipeline
    ("clf", RandomForestClassifier(n_estimators=300, random_state=42))
])

rf_pipeline.fit(X_train, y_train)

print("Random Forest Accuracy:", rf_pipeline.score(X_test, y_test))

"""**Step #13** constructs and trains a Random Forest‚Äìbased machine‚Äëlearning pipeline by combining the previously defined preprocessor with a RandomForestClassifier. The pipeline ensures that all numerical scaling and categorical encoding occur automatically before the model receives the data. The classifier is configured with 300 decision trees and a fixed random_state=42 for reproducibility. After fitting the pipeline on the training data, the model‚Äôs performance is evaluated on the test set, and the resulting accuracy score is printed. This step provides a more flexible, nonlinear alternative to logistic regression, allowing the workflow to compare model performance across different algorithms.

**(Why This Step Matters)**
This step matters because Random Forests introduce a powerful, nonlinear modeling approach that can capture complex relationships in the data that simpler models‚Äîlike logistic regression‚Äîmay miss. By embedding the classifier inside the same preprocessing pipeline, you guarantee that every tree in the forest receives consistently transformed data, eliminating the risk of mismatched preprocessing between training and prediction. Using many trees also stabilizes predictions and reduces overfitting, while the pipeline structure keeps the workflow clean, reproducible, and easy to extend or compare with other models.
"""

# 14. Extract and Visualize Feature Importances
importances = rf_pipeline.named_steps["clf"].feature_importances_

# Get feature names after preprocessing
transformed_feature_names = rf_pipeline.named_steps['preprocessor'].get_feature_names_out()

feat_imp = pd.DataFrame({"feature": transformed_feature_names, "importance": importances})
feat_imp = feat_imp.sort_values("importance", ascending=False)

plt.figure(figsize=(8, 6))
sns.barplot(data=feat_imp.head(15), x="importance", y="feature")
plt.title("Top 15 Feature Importances (Random Forest)")
plt.show()

"""**Step #14** extracts the feature importances learned by the Random Forest classifier and visualizes the top contributors to the model‚Äôs predictions. It begins by pulling the feature_importances_ values from the classifier inside the pipeline, then retrieves the transformed feature names produced by the preprocessing step. These are combined into a DataFrame, sorted from highest to lowest importance, and plotted using a horizontal bar chart to highlight the top 15 most influential features. This visualization provides a clear, intuitive understanding of which variables the model relies on most, making the Random Forest‚Äôs decision‚Äëmaking process more interpretable.

**(Why This Step Matters)**
This step matters because it transforms a complex ensemble model into something interpretable and actionable. Feature importance analysis helps you understand which variables drive the model‚Äôs predictions, guiding better feature engineering, model refinement, and domain insights. It can reveal unexpected patterns, highlight redundant or uninformative features, and support transparent communication with stakeholders. Visualizing these importances makes the model‚Äôs behavior easier to grasp, especially for non‚Äëtechnical audiences.
"""