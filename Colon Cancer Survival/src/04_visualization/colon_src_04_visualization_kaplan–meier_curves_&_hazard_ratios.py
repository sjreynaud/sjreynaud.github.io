# -*- coding: utf-8 -*-
"""Colon_src/04_visualization: Kaplan–Meier Curves & Hazard Ratios

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8hJnagSouoFXckVN42TX7NLQUSt4cgh
"""

# ------------------------------------------------------------
# Step 1: Load Core Libraries and Install Lifelines
# ------------------------------------------------------------
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler

!pip install lifelines
from lifelines import KaplanMeierFitter, CoxPHFitter

"""**Step #1** initializes the computational environment by loading all core Python libraries required for survival analysis, data handling, visualization, and preprocessing. This includes foundational packages such as pandas, matplotlib, seaborn, and scikit‑learn, followed by the installation and import of the lifelines library, which provides the Kaplan–Meier and Cox proportional hazards models used throughout the survival analysis workflow. By establishing these imports at the very beginning, the notebook ensures that all downstream steps—data cleaning, feature engineering, model fitting, and visualization—have the necessary tools available and ready to use.

**(Why This Step Matters)**
Survival analysis workflows depend on a specialized set of tools that are not included in standard Python installations. Installing and importing lifelines ensures that the notebook can compute survival functions, hazard ratios, and time‑to‑event models. Loading all libraries upfront also prevents runtime errors, promotes reproducibility, and makes the workflow easier for collaborators to follow. In short, this step establishes the technical foundation upon which every subsequent analysis step is built.
"""

# ------------------------------------------------------------
# Step 2: Mount Google Drive for Data Access
# ------------------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

"""**Step #2** connects your Colab notebook to your Google Drive so that datasets, scripts, and output files stored there can be accessed directly within the Colab environment. By mounting Google Drive, Colab creates a secure, temporary link between the notebook session and your Drive directory, allowing you to load large datasets, save processed files, and maintain a consistent project structure across sessions. This step ensures that all downstream analysis—such as survival modeling, preprocessing, and figure generation—can reliably pull data from a stable, persistent storage location rather than relying on Colab’s temporary runtime.

**(Why This Step Matters)**
Without mounting Google Drive, the notebook would have no stable location from which to load your colorectal cancer datasets or save analysis outputs. This step guarantees reproducibility, prevents data loss, and supports collaborative work by ensuring that all files live in a consistent, shared location. It also allows you to maintain versioned datasets and outputs, which is essential for clinical research pipelines where traceability and documentation matter.
"""

# ------------------------------------------------------------
# Step 3: Define Project Paths and Directory Structure
# ------------------------------------------------------------
project_root = "/content/drive/MyDrive/colorectal_cancer_prediction"
data_path = os.path.join(project_root, "data", "processed", "colorectal_cancer_prediction_cleaned_imputed.csv")
output_path = os.path.join(project_root, "outputs", "visualizations")

os.makedirs(output_path, exist_ok=True)

print("Data Path:", data_path)
print("Output Path:", output_path)

"""**Step #3** establishes the project’s directory structure by defining the root folder for your colorectal cancer prediction workflow and constructing standardized paths for both the cleaned dataset and the output visualizations. Using os.path.join, the code builds platform‑safe paths, and os.makedirs ensures that the output directory exists before any figures or model artifacts are saved. By printing the final paths, the notebook provides immediate confirmation that the workflow is correctly aligned with your Google Drive project layout, reducing the risk of file‑not‑found errors in later steps.

**(Why This Step Matters)**
A stable directory structure is the backbone of a reproducible analysis workflow. Without clearly defined paths, downstream steps—such as loading the cleaned dataset, saving Kaplan–Meier plots, or exporting model diagnostics—would fail or produce scattered outputs. This step ensures that every component of the pipeline knows exactly where to find inputs and where to store results, supporting long‑term maintainability, collaboration, and auditability.
"""

# ------------------------------------------------------------
# Step 4: Load Cleaned and Imputed Dataset”
# ------------------------------------------------------------
df = pd.read_csv(data_path)
df.head()

"""**Step #4** loads the cleaned and imputed colorectal cancer dataset into a pandas DataFrame using the file path defined earlier in the workflow. Once loaded, the notebook displays the first few rows of the dataset, allowing you to visually confirm that the data has been read correctly and that all expected variables, demographics, clinical history, treatment details, and survival‑related outcomes are present. This quick inspection ensures that the dataset is properly formatted and ready for downstream preprocessing, feature engineering, and survival modeling.

**(Why This Step Matters)**
Accurate data loading is foundational to any clinical research pipeline. If the dataset is missing, corrupted, or incorrectly formatted, every downstream analysis step will fail or produce misleading results. This step ensures that the cleaned and imputed dataset is accessible, intact, and aligned with the expected schema. It also gives collaborators and reviewers a transparent view of the dataset’s structure, supporting reproducibility and auditability both essential in medical research.
"""

# ------------------------------------------------------------
# Step 5: Standardize Column Names for Consistency
# ------------------------------------------------------------
df.columns = df.columns.str.strip().str.lower()
print("Columns:", df.columns.tolist())

"""**Step #5** standardizes all column names in the dataset by converting them to lowercase and removing any leading or trailing whitespace. This ensures that the DataFrame uses clean, predictable, and uniform column identifiers, which prevents errors caused by inconsistent capitalization or hidden spaces. After applying these transformations, the step prints the full list of cleaned column names, allowing you to verify that the dataset now follows a consistent naming convention suitable for analysis, modeling, and reproducible documentation.

**(Why This Step Matters)**
In clinical research pipelines, even small inconsistencies in column naming can lead to subtle errors, failed merges, or misaligned variables during modeling. Standardizing column names protects the workflow from these issues and supports reproducibility, especially when collaborating with others or integrating multiple datasets. This step also makes the codebase more maintainable by ensuring that all variables follow a predictable, machine‑friendly format.
"""

# ------------------------------------------------------------
# Step 6: Identify and Standardize Event and Duration Variables
# ------------------------------------------------------------
# Updated candidates to include actual column names from the dataset
event_candidates = ["event", "death", "death_y_n", "status", "event_status", "survival_status"]
duration_candidates = ["duration", "time", "survival_months", "fu_month", "followup_time", "time_to_recurrence"]

event_col = next((c for c in event_candidates if c in df.columns), None)
duration_col = next((c for c in duration_candidates if c in df.columns), None)

if event_col is None or duration_col is None:
    # If still not found, provide a more informative error or handle it differently
    raise ValueError(f"Could not find event/duration columns in dataset. Found event_col: {event_col}, duration_col: {duration_col}")

df = df.rename(columns={event_col: "event", duration_col: "duration"})

# Convert 'event' (survival_status) to numerical: 1 for Deceased, 0 for Survived
df["event"] = df["event"].apply(lambda x: 1 if x == "Deceased" else 0)
df["event"] = df["event"].astype(int)

# Ensure duration is numeric
df["duration"] = pd.to_numeric(df["duration"], errors='coerce')

"""**Step #6** identifies the correct event and duration columns required for survival analysis by scanning the dataset for known candidate names, renaming them to standardized labels, and converting their values into analysis‑ready formats. The code searches for any column that represents the event indicator (e.g., death, survival status) and any column that represents the time‑to‑event measure (e.g., follow‑up time, time to recurrence). Once found, these columns are renamed to "event" and "duration" for consistency, the event variable is converted into a binary numeric format (1 = Deceased, 0 = Survived), and the duration column is coerced into numeric values. This ensures that the dataset is properly aligned with the requirements of Kaplan–Meier and Cox proportional hazards models.

**(Why This Step Matters)**
Without correctly defined event and duration variables, survival models cannot run—and worse, they may run incorrectly if the wrong columns are used. This step protects the integrity of the analysis by validating the presence of required variables, standardizing their names, and ensuring they meet the expected data types. In clinical research, where survival outcomes are central to interpretation, this level of rigor is essential for reproducibility, accuracy, and trustworthiness of results.
"""

# ------------------------------------------------------------
# Step 7: Prepare Covariates for Cox Proportional Hazards Modeling
# ------------------------------------------------------------
covariates = [
    "age",
    "sex",
    "race",
    "tumor_stage",
    "tumor_grade",
    "cea_level",
    "bmi",
    "treatment_group"
]

existing_covariates = [c for c in covariates if c in df.columns]

cox_df = df[["duration", "event"] + existing_covariates].copy()

# Encode categorical variables
categorical_cols = ["sex", "race", "tumor_stage", "tumor_grade", "treatment_group"]
for col in categorical_cols:
    if col in cox_df.columns:
        le = LabelEncoder()
        cox_df[col] = le.fit_transform(cox_df[col])

# Scale numeric variables
numeric_cols = ["age", "cea_level", "bmi"]
for col in numeric_cols:
    if col in cox_df.columns:
        scaler = StandardScaler()
        cox_df[col] = scaler.fit_transform(cox_df[[col]])

"""**Step #7** prepares the covariates needed for the Cox proportional hazards model by selecting relevant clinical and demographic predictors, filtering them to include only those present in the dataset, and assembling them into a modeling‑ready DataFrame. The step encodes categorical variables using LabelEncoder to convert them into numerical form and scales continuous variables with StandardScaler to ensure they are on comparable ranges. By combining event, duration, and cleaned covariates into a single structured DataFrame, this step produces a fully preprocessed feature set suitable for fitting a Cox model without errors or inconsistencies.

**(Why This Step Matters)**
The Cox proportional hazards model is sensitive to variable formatting, and improperly prepared covariates can lead to model failures, convergence issues, or misleading hazard ratio estimates. By encoding categorical predictors, scaling continuous variables, and ensuring that only valid covariates are included, this step protects the integrity of the survival analysis. It also enhances reproducibility and transparency—critical qualities in clinical research—by clearly defining how each predictor is transformed before modeling.
"""

# ------------------------------------------------------------
# Step 8: Fit Cox Proportional Hazards Model
# ------------------------------------------------------------
cph = CoxPHFitter()
cph.fit(cox_df, duration_col="duration", event_col="event")

print("\nCox Model Summary:")
cph.print_summary()

"""**Step #8** fits the Cox Proportional Hazards (Cox PH) model using the fully prepared dataset containing the duration, event indicator, and cleaned covariates. The model is instantiated with CoxPHFitter() and trained on the processed DataFrame, after which a detailed summary is printed to display coefficients, hazard ratios, confidence intervals, significance levels, and model diagnostics such as concordance and log‑likelihood. This step marks the transition from data preparation to statistical inference, producing interpretable estimates that quantify how each covariate influences the hazard of the event occurring over time.

**(Why This Step Matters)**
This step is crucial because it produces the primary inferential results of the survival analysis. Without fitting the Cox model, you would have no quantitative estimates of how factors like age, sex, BMI, or tumor characteristics affect patient risk over time. The summary output also enables transparency and reproducibility, allowing collaborators, reviewers, or clinicians to evaluate the strength and direction of associations. In clinical research, these results often inform decision‑making, risk stratification, and future study design, making this step foundational to the entire workflow.
"""

# ------------------------------------------------------------
# Step 9: Stratify Patients Into Risk Tertiles
# ------------------------------------------------------------
cox_df["prognostic_score"] = cph.predict_partial_hazard(cox_df[existing_covariates])

tertiles = cox_df["prognostic_score"].quantile([1/3, 2/3])

def assign_group(score):
    if score <= tertiles.iloc[0]:
        return "Low"
    elif score <= tertiles.iloc[1]:
        return "Medium"
    else:
        return "High"

cox_df["risk_group"] = cox_df["prognostic_score"].apply(assign_group)

print("\nRisk Group Distribution:")
print(cox_df["risk_group"].value_counts())

"""**Step #9** stratifies patients into three risk tertiles based on their prognostic scores derived from the Cox model’s partial hazard predictions. After computing each patient’s prognostic score, the code determines the 1/3 and 2/3 quantile thresholds and assigns each patient to a Low, Medium, or High risk group accordingly. This creates a clinically interpretable risk‑stratification variable that summarizes the combined effect of all covariates in the Cox model. The step concludes by displaying the distribution of patients across the three groups, confirming that the tertiles are balanced and correctly assigned.

**(Why This Step Matters)**
Risk stratification is a cornerstone of clinical survival analysis because it translates complex multivariable predictions into actionable categories. Clinicians and researchers often rely on risk groups—not raw hazard scores—to guide decision‑making, identify high‑risk populations, and communicate findings. This step ensures that the model’s output becomes interpretable, reproducible, and suitable for visualization and clinical interpretation. Without this transformation, the prognostic score would remain abstract and difficult to use in practice.
"""

# ------------------------------------------------------------
# Step 10: Plot Kaplan–Meier Survival Curves by Risk Group
# ------------------------------------------------------------
kmf = KaplanMeierFitter()
plt.figure(figsize=(8,6))

for group in ["Low", "Medium", "High"]:
    mask = cox_df["risk_group"] == group
    if not cox_df[mask].empty:
        kmf.fit(cox_df[mask]["duration"], event_observed=cox_df[mask]["event"], label=group)
        kmf.plot_survival_function()

plt.title("Kaplan-Meier Survival by Risk Group")
plt.xlabel("Time (months)")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "km_risk_groups.png"))
plt.show()

"""**Step #10** generates Kaplan–Meier survival curves for the Low, Medium, and High risk groups created in the previous step. Using the KaplanMeierFitter, the code fits a separate survival function for each group and overlays the resulting curves on a single plot. This visualization highlights how survival probabilities differ across risk strata over time, making it easy to see whether higher‑risk patients experience earlier or more frequent events. The figure is then labeled, formatted, saved to the output directory, and displayed for interpretation.

**(Why This Step Matters)**
Kaplan–Meier plots are one of the most widely recognized tools in clinical research because they communicate survival differences in a clear, interpretable format. This step is essential for validating the prognostic value of the risk groups: if the curves separate cleanly, it supports the usefulness of the model; if they overlap, it signals that the stratification may need refinement. The visualization also serves as a key figure for reports, publications, and presentations, making it a critical component of the workflow.
"""

# ------------------------------------------------------------
# Step 11: Visualize Hazard Ratios from Cox Model
# ------------------------------------------------------------
plt.figure(figsize=(8,6))
cph.plot()
plt.title("Hazard Ratios from Cox Model")
plt.tight_layout()
plt.savefig(os.path.join(output_path, "hazard_ratios.png"))
plt.show()

"""**Step #11** creates a clear visual summary of the Cox model’s estimated hazard ratios by generating a plot that displays each covariate’s log‑hazard ratio along with its 95% confidence interval. Using the model’s built‑in plotting function, the code produces a figure that highlights which predictors increase or decrease risk and how precise those estimates are. The plot is formatted with a title, tightened layout, saved to the output directory, and displayed, providing an intuitive and interpretable representation of the model’s effect sizes.

**(Why This Step Matters)**
Hazard‑ratio visualizations are essential for communicating survival‑model results in a way that clinicians, collaborators, and stakeholders can quickly understand. They reveal the direction and magnitude of each covariate’s impact and highlight which predictors have statistically meaningful effects. Without this plot, interpretation would rely solely on numerical tables, which can obscure important patterns and make the results less accessible. This step strengthens transparency, interpretability, and the overall clarity of the analysis.
"""