# -*- coding: utf-8 -*-
"""Colon_src/03_evaluation â€” Calibration, Risk Stratification, Metrics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GE2p3mHIpqPaieki8ZuzsIs_3wx81YTa
"""

# ============================================================
# 1. Mount Google Drive
# ============================================================
from google.colab import drive
drive.mount('/content/drive')

# ============================================================
# 2. Import libraries and set paths
# ============================================================
import os
import pandas as pd
import numpy as np

from sklearn.metrics import (
    roc_auc_score,
    brier_score_loss,
    classification_report,
    confusion_matrix
)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.calibration import calibration_curve

!pip install lifelines
from lifelines.utils import concordance_index

# ============================================================
# 3. Define project paths
# ============================================================
project_root = "/content/drive/MyDrive/colorectal_cancer_prediction"

data_path = os.path.join(
    project_root,
    "data",
    "processed",
    "colorectal_cancer_prediction_cleaned_imputed.csv"
)

output_path = os.path.join(project_root, "outputs", "evaluation")
os.makedirs(output_path, exist_ok=True)

# ============================================================
# 4. Load dataset
# ============================================================
df = pd.read_csv(data_path)

print("Dataset loaded. Shape:", df.shape)
df.head()

# 5

print(df.columns)

#6.

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

class ClinicalModelingPipeline:
    def __init__(self, df, target_column):
        self.df = df
        self.target_column = target_column
        self.model = RandomForestClassifier(random_state=42)
        self.scaler = StandardScaler()
        print("ClinicalModelingPipeline initialized.")

    def split_data(self):
        X = self.df.drop(columns=[self.target_column])
        y = self.df[self.target_column]

        # Identify categorical columns for one-hot encoding if not already handled
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

        # Align columns after one-hot encoding for consistent scaling and training
        self.feature_columns = X.columns # Store feature columns after encoding

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y
        )

        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Convert back to DataFrame with original column names
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

        print("Data split and scaled successfully.")
        return X_train_scaled, X_test_scaled, y_train, y_test

    def train_model(self, X_train, y_train):
        self.model.fit(X_train, y_train)
        print("Model trained successfully.")
        return self.model

    def predict_proba(self, trained_model, X_test):
        y_pred_proba = trained_model.predict_proba(X_test)[:, 1]
        print("Predicted probabilities generated.")
        return y_pred_proba

print("ClinicalModelingPipeline class defined.")

# 7

df['Recurrence'] = df['Recurrence'].map({'No': 0, 'Yes': 1})

print("Recurrence column converted to numerical:")
print(df['Recurrence'].value_counts())
print("Data type of Recurrence column:", df['Recurrence'].dtype)

# 8

pipeline = ClinicalModelingPipeline(df, 'Recurrence')
X_train_scaled, X_test_scaled, y_train, y_test = pipeline.split_data()
trained_model = pipeline.train_model(X_train_scaled, y_train)
risk_score = pipeline.predict_proba(trained_model, X_test_scaled)

print("Shape of X_train_scaled:", X_train_scaled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
print("Length of risk_score:", len(risk_score))

#9 Calibration Curve

def plot_calibration_curve(y_true, y_prob, n_bins=10):
    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)

    plt.figure(figsize=(8, 6))
    plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')

    plt.xlabel("Predicted Probability")
    plt.ylabel("Observed Probability")
    plt.title("Calibration Curve")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()

    plt.savefig(os.path.join(output_path, "calibration_curve.png"))
    plt.show()

plot_calibration_curve(
    y_test,           # binary outcome column (true labels from test set)
    risk_score        # predicted probability column from test set
)

# ============================================================
# 10. Risk Stratification Summary
# ============================================================
def summarize_risk_groups(df_eval, time_col, event_col):

    summary = df_eval.groupby("risk_group", observed=False)[[time_col, event_col]].agg(
        ["count", "mean", "median"]
    )

    print("Risk Group Summary:")
    print(summary)

    summary.to_csv(os.path.join(output_path, "risk_group_summary.csv"))

# Create a DataFrame for evaluation that includes true labels and predicted risk scores for the test set
df_eval = pd.DataFrame({
    'y_true': y_test,
    'risk_score': risk_score
}, index=y_test.index)

# Create risk groups based on quartiles of the predicted risk scores
df_eval["risk_group"] = pd.qcut(
    df_eval["risk_score"],
    q=4,
    labels=["Q1", "Q2", "Q3", "Q4"]
)

# The original df contains 'Time_to_Recurrence' and 'Recurrence' which can be joined with df_eval
# to get the necessary time_col and event_col data.
# For this example, let's assume 'Time_to_Recurrence' and 'Recurrence' are the desired columns
# and that y_test (Recurrence) is the 'event_col'.
# Let's map Time_to_Recurrence from the original df to df_eval using the index.
df_eval = df_eval.join(df[['Time_to_Recurrence']], how='left')

# Summarize risk groups using the relevant columns from df_eval
summarize_risk_groups(
    df_eval,
    time_col="Time_to_Recurrence",
    event_col="y_true" # y_true contains the binary 'Recurrence' outcome for the test set
)

# ============================================================
# 11. Evaluation Metrics
# ============================================================
def evaluate_model(df_to_eval, y_true_col, y_time_col, risk_score_col):

    y_true = df_to_eval[y_true_col]
    y_time = df_to_eval[y_time_col]
    risk_scores = df_to_eval[risk_score_col]

    # Concordance Index
    c_index = concordance_index(y_time, -risk_scores, y_true)
    print(f"Concordance Index: {c_index:.3f}")

    # AUC
    auc = roc_auc_score(y_true, risk_scores)
    print(f"AUC: {auc:.3f}")

    # Brier Score
    brier = brier_score_loss(y_true, risk_scores)
    print(f"Brier Score: {brier:.3f}")

    return c_index, auc, brier

# Run evaluation using the df_eval DataFrame from the previous step
c_index, auc, brier = evaluate_model(
    df_eval,
    y_true_col="y_true", # Use 'y_true' from df_eval which is the binary Recurrence outcome
    y_time_col="Time_to_Recurrence", # Use 'Time_to_Recurrence' from df_eval
    risk_score_col="risk_score" # Use 'risk_score' from df_eval
)

# ============================================================
# 12. Optional: Dummy risk score (remove when using real model)
# ============================================================
df["risk_score"] = 1 / (1 + df["Time_to_Recurrence"])

c_index, auc, brier = evaluate_model(
    df,
    y_true_col="Recurrence",
    y_time_col="Time_to_Recurrence",
    risk_score_col="risk_score"
)

print("\nEvaluation Metrics using dummy risk scores:")
print(f"  Concordance Index: {c_index:.3f}")
print(f"  AUC: {auc:.3f}")
print(f"  Brier Score: {brier:.3f}")