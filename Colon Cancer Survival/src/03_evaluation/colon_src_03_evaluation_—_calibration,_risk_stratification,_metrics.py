# -*- coding: utf-8 -*-
"""Colon_src/03_evaluation — Calibration, Risk Stratification, Metrics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GE2p3mHIpqPaieki8ZuzsIs_3wx81YTa
"""

# ============================================================
# 1. Connect to Google Drive for File Access
# ============================================================
from google.colab import drive
drive.mount('/content/drive')

"""**Step #1** initializes access to your Google Drive within the Colab environment by importing the google.colab.drive module and mounting your Drive at the /content/drive directory. Once mounted, Colab treats your Drive like a local filesystem, allowing you to load datasets, save outputs, and maintain persistent files across sessions. The output message simply confirms that the Drive is already mounted and reminds you how to forcibly remount it if needed.

**(Why This Step Matters)**
Mounting Google Drive is essential because it provides stable, persistent storage for your project. Colab’s temporary runtime resets frequently, but your Drive does not, so mounting ensures that datasets, intermediate outputs, models, and documentation remain accessible across sessions. Without this step, your workflow would be limited to Colab’s ephemeral storage, making reproducibility and long‑term project organization nearly impossible.


"""

# ============================================================
# 2. Load Required Libraries and Initialize Analysis Environment
# ============================================================
import os
import pandas as pd
import numpy as np

from sklearn.metrics import (
    roc_auc_score,
    brier_score_loss,
    classification_report,
    confusion_matrix
)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.calibration import calibration_curve

!pip install lifelines
from lifelines.utils import concordance_index

"""**Step #2** loads all essential Python libraries and sets up the computational environment needed for data processing, visualization, model evaluation, and survival analysis. This includes core packages like pandas and numpy for data manipulation, matplotlib and seaborn for plotting, and scikit‑learn metrics for evaluating model performance. The step also installs and imports the lifelines package, which provides tools such as the concordance index for survival modeling. Together, these imports establish the analytical toolkit that the rest of the workflow depends on.

**(Why This Step Matters)**
This step is foundational because every downstream operation loading data, generating figures, computing metrics, building models, or performing survival analysis relies on these libraries being available and correctly imported. Installing lifelines ensures that specialized survival‑analysis functions are accessible, while scikit‑learn and visualization libraries provide the backbone for evaluation and interpretation. Without this setup, the workflow would break immediately due to missing tools or undefined functions.
"""

# ============================================================
# 3. Set Project Directory Structure and File Paths
# ============================================================
project_root = "/content/drive/MyDrive/colorectal_cancer_prediction"

data_path = os.path.join(
    project_root,
    "data",
    "processed",
    "colorectal_cancer_prediction_cleaned_imputed.csv"
)

output_path = os.path.join(project_root, "outputs", "evaluation")
os.makedirs(output_path, exist_ok=True)

"""**Step #3** establishes the directory structure that the rest of the workflow will rely on by defining the project’s root folder, constructing the full path to the cleaned and imputed colorectal cancer dataset, and creating an output directory for evaluation results. By using os.path.join, the paths are built in a platform‑safe way, and os.makedirs(..., exist_ok=True) ensures that the output directory is created if it doesn’t already exist. This step centralizes all file locations, making the workflow easier to maintain, reuse, and adapt.

**(Why This Step Matters)**
Defining project paths early prevents confusion, broken links, and hard‑coded file locations scattered throughout the notebook. It ensures that every component of the workflow, data loading, model outputs, evaluation plots, and saved artifacts points to a predictable, well‑organized location. This is especially important for reproducibility, collaboration, and long‑term maintenance, where clarity in file structure directly impacts the reliability of the entire pipeline.
"""

# ============================================================
# 4. Load and Inspect the Dataset
# ============================================================
df = pd.read_csv(data_path)

print("Dataset loaded. Shape:", df.shape)
df.head()

"""**Step #4** loads the cleaned and imputed colorectal cancer dataset into a pandas DataFrame using the file path defined earlier. After reading the CSV, the code prints the dataset’s shape to confirm the number of rows and columns, then displays the first few records to provide a quick visual check of the data structure, variable names, and sample values. This immediate preview helps verify that the dataset was loaded correctly and that the expected features are present before moving on to further analysis.

**(Why This Step Matters)**
Loading the dataset is a foundational moment in the pipeline because every The The subsequent step, cleaning, feature engineering, modeling, and evaluation, depends on having the correct data available and correctly structured. Displaying the shape and first few rows acts as an early validation checkpoint, catching issues such as incorrect file paths, missing columns, corrupted data, or unexpected formatting. Ensuring the dataset is properly loaded prevents downstream errors and supports reproducibility and transparency.
"""

# ============================================================
# 5 Inspect Dataset Columns
# ============================================================

print(df.columns)

"""**Step #5** prints the full list of column names in the dataset, giving you a clear overview of all available variables before any preprocessing or modeling begins. By displaying the DataFrame’s schema, you can quickly verify that the expected features are present, confirm naming consistency, and identify potential issues such as typos, missing fields, or unexpected columns. This simple but essential inspection step ensures you understand the structure and scope of the dataset you’re about to analyze.

**(Why This Step Matters)**
Knowing exactly which columns exist and how they are named is crucial for building a reliable pipeline. Many errors in data analysis stem from incorrect column references, inconsistent naming conventions, or assumptions about available features. By explicitly printing the column list early, you create a checkpoint that helps prevent downstream bugs, supports reproducibility, and ensures that subsequent steps (such as selecting predictors, engineering features, or defining targets) are grounded in an accurate understanding of the dataset.
"""

# ============================================================
#6.Build the Clinical Modeling Pipeline Class
# ============================================================

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

class ClinicalModelingPipeline:
    def __init__(self, df, target_column):
        self.df = df
        self.target_column = target_column
        self.model = RandomForestClassifier(random_state=42)
        self.scaler = StandardScaler()
        print("ClinicalModelingPipeline initialized.")

    def split_data(self):
        X = self.df.drop(columns=[self.target_column])
        y = self.df[self.target_column]

        # Identify categorical columns for one-hot encoding if not already handled
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

        # Align columns after one-hot encoding for consistent scaling and training
        self.feature_columns = X.columns # Store feature columns after encoding

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y
        )

        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Convert back to DataFrame with original column names
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

        print("Data split and scaled successfully.")
        return X_train_scaled, X_test_scaled, y_train, y_test

    def train_model(self, X_train, y_train):
        self.model.fit(X_train, y_train)
        print("Model trained successfully.")
        return self.model

    def predict_proba(self, trained_model, X_test):
        y_pred_proba = trained_model.predict_proba(X_test)[:, 1]
        print("Predicted probabilities generated.")
        return y_pred_proba

print("ClinicalModelingPipeline class defined.")

"""**Step #6** defines the ClinicalModelingPipeline class, which encapsulates the full machine‑learning workflow from preprocessing to model training and prediction into a clean, reusable structure. The class initializes a RandomForestClassifier, sets up a StandardScaler, and provides methods for splitting the data, handling categorical variables through one‑hot encoding, scaling features, training the model, and generating predicted probabilities. By organizing these operations into a single, well‑structured class, the workflow becomes modular, maintainable, and easy to extend for future experiments or clinical modeling tasks.

**(Why This Step Matters)**
This step is crucial because it transforms a loose collection of modeling operations into a coherent, reusable pipeline. By encapsulating data splitting, encoding, scaling, and model training inside a class, you ensure consistency across experiments, reduce code duplication, and make the workflow easier to debug and maintain. This structure also supports reproducibility an essential requirement in clinical research because every modeling step is explicitly defined, controlled, and repeatable. As your project grows, this pipeline becomes the backbone that keeps the analysis organized and scalable.
"""

# ============================================================
# 7 Encode Recurrence as a Binary Variable
# ============================================================

df['Recurrence'] = df['Recurrence'].map({'No': 0, 'Yes': 1})

print("Recurrence column converted to numerical:")
print(df['Recurrence'].value_counts())
print("Data type of Recurrence column:", df['Recurrence'].dtype)

"""**Step #7** converts the Recurrence column from categorical text labels (“Yes” and “No”) into numerical values (1 and 0), making the variable compatible with machine‑learning algorithms that require numeric inputs. After applying the mapping, the code prints the value counts to confirm the transformation and displays the resulting data type to ensure the column is now stored as integers. This quick verification step confirms that the encoding worked correctly and that the dataset is ready for downstream modeling.

**(Why This Step Matters)**
Most machine‑learning models cannot directly interpret text‑based categorical values, especially for binary outcomes like recurrence. Converting “Yes/No” into 1/0 ensures the model can learn from this variable and treat it as a proper target or feature. This step also standardizes the data, reduces ambiguity, and prevents errors during training. By validating the counts and data type immediately, you create a built‑in quality check that safeguards the integrity of the modeling pipeline.
"""

# ============================================================
# 8 Execute Pipeline: Train Model and Generate Risk Scores
# ============================================================

pipeline = ClinicalModelingPipeline(df, 'Recurrence')
X_train_scaled, X_test_scaled, y_train, y_test = pipeline.split_data()
trained_model = pipeline.train_model(X_train_scaled, y_train)
risk_score = pipeline.predict_proba(trained_model, X_test_scaled)

print("Shape of X_train_scaled:", X_train_scaled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
print("Length of risk_score:", len(risk_score))

"""**Step #8** puts the entire modeling pipeline into action by instantiating the ClinicalModelingPipeline class, splitting and scaling the data, training the Random Forest model, and generating predicted probabilities (risk scores) for the test set. The printed shapes of the training and testing datasets confirm that the split was performed correctly, and the length of the risk score array verifies that predictions were generated for every test instance. This step marks the transition from data preparation to actual model execution, producing the core output needed for evaluation.

**(Why This Step Matters)**
This step is essential because it operationalizes the modeling pipeline and produces the risk scores that downstream evaluation metrics depend on. Without running the pipeline, there would be no trained model, no predictions, and no way to assess performance. The printed shapes serve as built‑in validation checkpoints, ensuring that the data was processed correctly and that the model generated outputs of the expected size. This step effectively bridges the gap between pipeline design and real analytical results.
"""

# ============================================================
#9 Plot and Evaluate Model Calibration
# ============================================================

def plot_calibration_curve(y_true, y_prob, n_bins=10):
    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)

    plt.figure(figsize=(8, 6))
    plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')

    plt.xlabel("Predicted Probability")
    plt.ylabel("Observed Probability")
    plt.title("Calibration Curve")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()

    plt.savefig(os.path.join(output_path, "calibration_curve.png"))
    plt.show()

plot_calibration_curve(
    y_test,           # binary outcome column (true labels from test set)
    risk_score        # predicted probability column from test set
)

"""**Step #9** generates a calibration curve to evaluate how well the model’s predicted probabilities align with actual observed outcomes. The function computes the relationship between predicted risk scores and true event rates across bins, then plots both the model’s calibration line and a reference line representing perfect calibration. The resulting visualization provides an intuitive assessment of whether the model tends to overestimate or underestimate risk, and the plot is saved to the project’s output directory for documentation and reporting.

**(Why This Step Matters)**
Calibration is a critical component of clinical prediction modeling because a model must not only discriminate between outcomes but also provide trustworthy probability estimates. A well‑calibrated model ensures that predicted risks correspond meaningfully to actual event rates, del ensures that predicted risks correspond meaningfully to actual event rates an essential requirement when predictions may influence clinical decisions, patient counseling, or resource allocation. By plotting the calibration curve, this step provides a transparent, visual check on probability accuracy and helps identify whether the model systematically over‑ or under‑predicts risk.
"""

# ============================================================
# 10. Stratify Patients by Risk and Summarize Outcomes
# ============================================================
def summarize_risk_groups(df_eval, time_col, event_col):

    summary = df_eval.groupby("risk_group", observed=False)[[time_col, event_col]].agg(
        ["count", "mean", "median"]
    )

    print("Risk Group Summary:")
    print(summary)

    summary.to_csv(os.path.join(output_path, "risk_group_summary.csv"))

# Create a DataFrame for evaluation that includes true labels and predicted risk scores for the test set
df_eval = pd.DataFrame({
    'y_true': y_test,
    'risk_score': risk_score
}, index=y_test.index)

# Create risk groups based on quartiles of the predicted risk scores
df_eval["risk_group"] = pd.qcut(
    df_eval["risk_score"],
    q=4,
    labels=["Q1", "Q2", "Q3", "Q4"]
)

# The original df contains 'Time_to_Recurrence' and 'Recurrence' which can be joined with df_eval
# to get the necessary time_col and event_col data.
# For this example, let's assume 'Time_to_Recurrence' and 'Recurrence' are the desired columns
# and that y_test (Recurrence) is the 'event_col'.
# Let's map Time_to_Recurrence from the original df to df_eval using the index.
df_eval = df_eval.join(df[['Time_to_Recurrence']], how='left')

# Summarize risk groups using the relevant columns from df_eval
summarize_risk_groups(
    df_eval,
    time_col="Time_to_Recurrence",
    event_col="y_true" # y_true contains the binary 'Recurrence' outcome for the test set
)

"""**Step #10** performs a full risk‑stratification analysis by grouping patients into quartiles based on their predicted risk scores and summarizing key clinical outcomes within each group. The code creates an evaluation DataFrame containing true outcomes, predicted probabilities, and assigned risk groups, then merges in the relevant time‑to‑event information. The summarize_risk_groups function computes counts, means, and medians for both the event indicator and the time‑to‑recurrence variable across the four risk strata. The resulting summary provides a structured view of how predicted risk aligns with actual clinical outcomes, and the results are saved for reporting.

**(Why This Step Matters)**
Risk stratification is a cornerstone of clinical prediction modeling because it translates raw model outputs into clinically interpretable categories. By dividing patients into quartiles and examining how outcomes vary across these groups, you can assess whether the model meaningfully separates low‑risk from high‑risk individuals. This step also provides essential descriptive statistics that help validate the model’s usefulness, support clinical decision‑making, and prepare results for publication or stakeholder communication. Saving the summary ensures reproducibility and creates a permanent record of the model’s performance across risk tiers.
"""

# ============================================================
# 11. Compute Core Model Performance Metrics
# ============================================================
def evaluate_model(df_to_eval, y_true_col, y_time_col, risk_score_col):

    y_true = df_to_eval[y_true_col]
    y_time = df_to_eval[y_time_col]
    risk_scores = df_to_eval[risk_score_col]

    # Concordance Index
    c_index = concordance_index(y_time, -risk_scores, y_true)
    print(f"Concordance Index: {c_index:.3f}")

    # AUC
    auc = roc_auc_score(y_true, risk_scores)
    print(f"AUC: {auc:.3f}")

    # Brier Score
    brier = brier_score_loss(y_true, risk_scores)
    print(f"Brier Score: {brier:.3f}")

    return c_index, auc, brier

# Run evaluation using the df_eval DataFrame from the previous step
c_index, auc, brier = evaluate_model(
    df_eval,
    y_true_col="y_true", # Use 'y_true' from df_eval which is the binary Recurrence outcome
    y_time_col="Time_to_Recurrence", # Use 'Time_to_Recurrence' from df_eval
    risk_score_col="risk_score" # Use 'risk_score' from df_eval
)

"""**Step #11** evaluates the predictive performance of the model using three complementary metrics: the Concordance Index, AUC, and Brier Score. The function extracts the true outcomes, time‑to‑event values, and predicted risk scores from the evaluation DataFrame, then computes each metric to quantify different aspects of model quality. The Concordance Index measures how well the model ranks patients by risk, the AUC assesses discrimination between recurrence and non‑recurrence, and the Brier Score evaluates the accuracy of the predicted probabilities. The results are printed and returned, providing a concise performance summary for the model.

**(Why This Step Matters)**
Evaluation metrics are essential for determining whether a predictive model is clinically meaningful and statistically reliable. Each metric captures a different dimension of performance: the Concordance Index reflects ranking ability in a time‑to‑event context, AUC measures classification discrimination, and the Brier Score assesses probability calibration and overall prediction accuracy. Together, they provide a multidimensional assessment of model quality, helping you understand strengths, weaknesses, and whether the model is suitable for real‑world use or requires further refinement.
"""

# ============================================================
# 12. Create and Evaluate a Baseline Risk Model
# ============================================================
df["risk_score"] = 1 / (1 + df["Time_to_Recurrence"])

c_index, auc, brier = evaluate_model(
    df,
    y_true_col="Recurrence",
    y_time_col="Time_to_Recurrence",
    risk_score_col="risk_score"
)

print("\nEvaluation Metrics using dummy risk scores:")
print(f"  Concordance Index: {c_index:.3f}")
print(f"  AUC: {auc:.3f}")
print(f"  Brier Score: {brier:.3f}")

"""**Step #12** creates and evaluates a simple dummy risk score to serve as a baseline comparison for the main predictive model. The dummy score is computed using a straightforward formula 1/(1+Time_to_Recurrence), score is computed using a straightforward formula 1/(1+Time_to_Recurrence) which produces higher risk values for shorter recurrence times. This synthetic risk score is then passed through the same evaluation function used for the primary model, generating Concordance Index, AUC, and Brier Score values. These metrics reveal how a naïve, non‑learned risk estimate performs, providing a reference point for interpreting the quality of the trained model.

**(Why This Step Matters)**
Baseline models are essential for determining whether a trained machine‑learning model provides meaningful improvement over trivial or heuristic approaches. By evaluating a dummy risk score, you establish a performance floor showing what can be achieved without learning from the data. If the trained model does not outperform this baseline, it signals potential issues such as poor feature selection, model misspecification, or data limitations. This step strengthens the scientific rigor of the workflow by ensuring that model performance is interpreted in context rather than in isolation.
"""